{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.6: Control Experiment - The Impact of Data Balancing\n",
    "\n",
    "**Objective:** This notebook serves as a crucial scientific control experiment (an ablation study). After achieving peak performance with our \"Ultimate Generalist\" model (v5) which was trained on a balanced dataset, we now test the hypothesis that data balancing was a key factor in its success.\n",
    "\n",
    "To do this, we will run the exact same advanced training regimen, but on the original, **unbalanced** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30743,
     "status": "ok",
     "timestamp": 1756978249868,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "g75TgZwtC9wG",
    "outputId": "8911ca5d-e9b3-4023-c01b-2b36360786b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Collecting audiomentations\n",
      "  Downloading audiomentations-0.42.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
      "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
      "  Downloading numpy_minmax-0.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
      "  Downloading numpy_rms-0.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting python-stretch<1,>=0.3.1 (from audiomentations)\n",
      "  Downloading python_stretch-0.3.1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n",
      "Downloading audiomentations-0.42.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.5/86.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy_minmax-0.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading numpy_rms-0.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18 kB)\n",
      "Downloading python_stretch-0.3.1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-stretch, numpy-rms, numpy-minmax, audiomentations\n",
      "Successfully installed audiomentations-0.42.0 numpy-minmax-0.5.0 numpy-rms-0.6.0 python-stretch-0.3.1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Connect to Google Drive to access your files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 2: Install all the necessary special libraries for our project\n",
    "# This single line installs everything we need.\n",
    "!pip install librosa audiomentations pandas seaborn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33632,
     "status": "ok",
     "timestamp": 1756978301806,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "-0hLADIsDEaG",
    "outputId": "7221a92e-7801-48c1-c51d-217ee102947c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for folder at: /content/drive/MyDrive/ser_project/processed_spectrograms_final/\n",
      "\n",
      "✅ Success! Found the folder.\n",
      "Total files found in the folder: 8882\n",
      "\n",
      "Here are the first 10 filenames found:\n",
      "03-01-01-01-01-01-01.npy\n",
      "03-01-01-01-01-01-02.npy\n",
      "03-01-01-01-01-01-03.npy\n",
      "03-01-01-01-01-01-04.npy\n",
      "03-01-01-01-01-01-05.npy\n",
      "03-01-01-01-01-01-06.npy\n",
      "03-01-01-01-01-01-07.npy\n",
      "03-01-01-01-01-01-08.npy\n",
      "03-01-01-01-01-01-09.npy\n",
      "03-01-01-01-01-01-10.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The path we expect the files to be in\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms_final/\"\n",
    "\n",
    "print(f\"Checking for folder at: {SPECTROGRAM_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Get a list of all files in that directory\n",
    "    all_npy_files = os.listdir(SPECTROGRAM_PATH)\n",
    "\n",
    "    print(f\"\\n✅ Success! Found the folder.\")\n",
    "    print(f\"Total files found in the folder: {len(all_npy_files)}\")\n",
    "\n",
    "    if len(all_npy_files) > 0:\n",
    "        print(\"\\nHere are the first 10 filenames found:\")\n",
    "        # Print the first 10 filenames for us to inspect\n",
    "        for filename in sorted(all_npy_files)[:10]:\n",
    "            print(filename)\n",
    "    else:\n",
    "        print(\"\\nWARNING: The folder is empty!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n❌ ERROR: The folder '{SPECTrogram_PATH}' does not exist.\")\n",
    "    print(\"Please double-check that you uploaded the folder and that the name is spelled exactly correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training on an Unbalanced Dataset\n",
    "\n",
    "The core of this experiment is a single, deliberate change from our champion v5 model:\n",
    "\n",
    "* **Data Strategy:** Instead of loading our balanced data splits, we are loading the `_unbalanced.pkl` file lists. This means the model will see a natural distribution of data, which is heavily skewed towards the larger CREMA-D dataset.\n",
    "\n",
    "All other advanced techniques (`SpecAugment`, `CosineAnnealingLR` scheduler, `ResNet18` architecture) are kept identical to ensure a fair, apples-to-apples comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 86359,
     "status": "ok",
     "timestamp": 1756979556491,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "QBJ57_wpDG-1",
    "outputId": "bb737c94-c98b-4845-af87-30824fb33e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading pre-defined and balanced data splits...\n",
      "Verifying that all spectrogram files exist...\n",
      "Train set: 6500 files found, 0 skipped.\n",
      "Validation set: 723 files found, 0 skipped.\n",
      "Test set: 1275 files found, 0 skipped.\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 189MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting advanced training with SpecAugment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 [Train]: 100%|██████████| 102/102 [03:23<00:00,  1.99s/it]\n",
      "Epoch 1/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 1.3750 | Val Loss: 1.7702 | Val Acc: 34.30%\n",
      "🎉 New best validation accuracy: 34.30%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 [Train]: 100%|██████████| 102/102 [00:25<00:00,  3.93it/s]\n",
      "Epoch 2/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 | Train Loss: 1.1496 | Val Loss: 4.8484 | Val Acc: 20.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.43it/s]\n",
      "Epoch 3/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 | Train Loss: 1.0465 | Val Loss: 2.1026 | Val Acc: 36.65%\n",
      "🎉 New best validation accuracy: 36.65%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.18it/s]\n",
      "Epoch 4/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 | Train Loss: 0.9581 | Val Loss: 1.3923 | Val Acc: 52.84%\n",
      "🎉 New best validation accuracy: 52.84%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.15it/s]\n",
      "Epoch 5/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 | Train Loss: 0.8403 | Val Loss: 1.3017 | Val Acc: 52.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.48it/s]\n",
      "Epoch 6/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 | Train Loss: 0.7088 | Val Loss: 1.7608 | Val Acc: 48.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.53it/s]\n",
      "Epoch 7/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 | Train Loss: 0.6122 | Val Loss: 2.5896 | Val Acc: 37.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.53it/s]\n",
      "Epoch 8/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 | Train Loss: 0.5002 | Val Loss: 1.8277 | Val Acc: 47.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.35it/s]\n",
      "Epoch 9/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 | Train Loss: 0.3765 | Val Loss: 2.0300 | Val Acc: 48.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.33it/s]\n",
      "Epoch 10/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 | Train Loss: 0.2830 | Val Loss: 2.7965 | Val Acc: 43.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.34it/s]\n",
      "Epoch 11/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 | Train Loss: 0.1993 | Val Loss: 3.1540 | Val Acc: 44.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.35it/s]\n",
      "Epoch 12/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 | Train Loss: 0.2137 | Val Loss: 2.9890 | Val Acc: 46.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.31it/s]\n",
      "Epoch 13/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 | Train Loss: 0.1449 | Val Loss: 2.0428 | Val Acc: 57.68%\n",
      "🎉 New best validation accuracy: 57.68%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.10it/s]\n",
      "Epoch 14/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 | Train Loss: 0.0870 | Val Loss: 1.7184 | Val Acc: 59.61%\n",
      "🎉 New best validation accuracy: 59.61%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.10it/s]\n",
      "Epoch 15/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 | Train Loss: 0.1001 | Val Loss: 2.1175 | Val Acc: 55.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.30it/s]\n",
      "Epoch 16/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 | Train Loss: 0.0706 | Val Loss: 1.7621 | Val Acc: 61.55%\n",
      "🎉 New best validation accuracy: 61.55%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.09it/s]\n",
      "Epoch 17/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 | Train Loss: 0.0491 | Val Loss: 2.1024 | Val Acc: 55.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.35it/s]\n",
      "Epoch 18/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 | Train Loss: 0.0430 | Val Loss: 1.9732 | Val Acc: 60.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.44it/s]\n",
      "Epoch 19/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 | Train Loss: 0.0512 | Val Loss: 1.6761 | Val Acc: 62.52%\n",
      "🎉 New best validation accuracy: 62.52%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 [Train]: 100%|██████████| 102/102 [00:25<00:00,  4.08it/s]\n",
      "Epoch 20/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 | Train Loss: 0.0531 | Val Loss: 1.9814 | Val Acc: 57.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.49it/s]\n",
      "Epoch 21/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 | Train Loss: 0.0342 | Val Loss: 1.8638 | Val Acc: 60.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.56it/s]\n",
      "Epoch 22/40 [Val]: 100%|██████████| 12/12 [00:03<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 | Train Loss: 0.0211 | Val Loss: 1.8742 | Val Acc: 61.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.54it/s]\n",
      "Epoch 23/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 | Train Loss: 0.0144 | Val Loss: 1.7527 | Val Acc: 62.79%\n",
      "🎉 New best validation accuracy: 62.79%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.19it/s]\n",
      "Epoch 24/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 | Train Loss: 0.0183 | Val Loss: 1.7898 | Val Acc: 63.35%\n",
      "🎉 New best validation accuracy: 63.35%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.15it/s]\n",
      "Epoch 25/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 | Train Loss: 0.0127 | Val Loss: 1.6808 | Val Acc: 65.01%\n",
      "🎉 New best validation accuracy: 65.01%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.15it/s]\n",
      "Epoch 26/40 [Val]: 100%|██████████| 12/12 [00:03<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 | Train Loss: 0.0114 | Val Loss: 1.7095 | Val Acc: 61.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.46it/s]\n",
      "Epoch 27/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 | Train Loss: 0.0103 | Val Loss: 1.7124 | Val Acc: 63.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.39it/s]\n",
      "Epoch 28/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 | Train Loss: 0.0089 | Val Loss: 1.7997 | Val Acc: 63.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.32it/s]\n",
      "Epoch 29/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 | Train Loss: 0.0069 | Val Loss: 1.8133 | Val Acc: 63.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.36it/s]\n",
      "Epoch 30/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 | Train Loss: 0.0059 | Val Loss: 1.7689 | Val Acc: 64.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.34it/s]\n",
      "Epoch 31/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 | Train Loss: 0.0057 | Val Loss: 1.7713 | Val Acc: 62.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.37it/s]\n",
      "Epoch 32/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 | Train Loss: 0.0076 | Val Loss: 1.7794 | Val Acc: 64.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.31it/s]\n",
      "Epoch 33/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 | Train Loss: 0.0063 | Val Loss: 1.7481 | Val Acc: 64.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.38it/s]\n",
      "Epoch 34/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 | Train Loss: 0.0051 | Val Loss: 1.7796 | Val Acc: 63.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 [Train]: 100%|██████████| 102/102 [00:23<00:00,  4.41it/s]\n",
      "Epoch 35/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 | Train Loss: 0.0071 | Val Loss: 1.7725 | Val Acc: 64.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.55it/s]\n",
      "Epoch 36/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 | Train Loss: 0.0046 | Val Loss: 1.7269 | Val Acc: 64.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.56it/s]\n",
      "Epoch 37/40 [Val]: 100%|██████████| 12/12 [00:03<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 | Train Loss: 0.0070 | Val Loss: 1.7555 | Val Acc: 65.15%\n",
      "🎉 New best validation accuracy: 65.15%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 [Train]: 100%|██████████| 102/102 [00:24<00:00,  4.22it/s]\n",
      "Epoch 38/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 | Train Loss: 0.0051 | Val Loss: 1.7656 | Val Acc: 64.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.52it/s]\n",
      "Epoch 39/40 [Val]: 100%|██████████| 12/12 [00:03<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 | Train Loss: 0.0043 | Val Loss: 1.7094 | Val Acc: 64.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 [Train]: 100%|██████████| 102/102 [00:22<00:00,  4.51it/s]\n",
      "Epoch 40/40 [Val]: 100%|██████████| 12/12 [00:02<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 | Train Loss: 0.0051 | Val Loss: 1.7014 | Val Acc: 65.15%\n",
      "\n",
      "--- FINAL EVALUATION OF ADVANCED GENERALIST MODEL ---\n",
      "Loading best model (from epoch with 65.15% validation accuracy) for final testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 20/20 [00:04<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Advanced Generalist Model Accuracy on the Test Set: 66.75%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.71      0.77      0.74       177\n",
      "       happy       0.67      0.69      0.68       214\n",
      "         sad       0.65      0.58      0.61       236\n",
      "       angry       0.72      0.76      0.74       217\n",
      "     fearful       0.65      0.60      0.62       215\n",
      "     disgust       0.61      0.63      0.62       216\n",
      "\n",
      "    accuracy                           0.67      1275\n",
      "   macro avg       0.67      0.67      0.67      1275\n",
      "weighted avg       0.67      0.67      0.67      1275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ULTIMATE COLAB SCRIPT v8: The Advanced Generalist Trainer (FINAL PATH FIX 3)\n",
    "# ===================================================================\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, numpy as np, pickle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Configuration ---\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms_final/\"\n",
    "FILE_LIST_PATH = \"/content/drive/MyDrive/ser_project/\"\n",
    "LEARNING_RATE = 0.001; BATCH_SIZE = 64; EPOCHS = 40\n",
    "CHECKPOINT_BEST_PATH = \"/content/drive/MyDrive/ser_project/resnet_advanced_best.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Mappings ---\n",
    "unified_emotion_map = { \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3, \"fearful\": 4, \"disgust\": 5 }\n",
    "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
    "\n",
    "# --- SpecAugment Transformation Pipeline ---\n",
    "spec_augment_transform = transforms.Compose([\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.05), ratio=(0.2, 5.0), value=0),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.08), ratio=(0.01, 0.2), value=0),\n",
    "])\n",
    "\n",
    "# --- Helper function to get filename robustly ---\n",
    "def get_basename(path):\n",
    "    return path.replace('\\\\', '/').split('/')[-1]\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300):\n",
    "        self.file_paths, self.labels, self.target_width = file_paths, labels, target_width\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        # Use our new robust function to get the base filename\n",
    "        filename = get_basename(self.file_paths[idx]).replace('.wav', '.npy')\n",
    "        file_path = os.path.join(SPECTROGRAM_PATH, filename)\n",
    "        label = self.labels[idx]\n",
    "        spectrogram = np.load(file_path)\n",
    "        current_width = spectrogram.shape[1]\n",
    "        if current_width < self.target_width: spectrogram = np.pad(spectrogram, ((0, 0), (0, self.target_width - current_width)), mode='constant')\n",
    "        elif current_width > self.target_width: spectrogram = spectrogram[:, :self.target_width]\n",
    "        spec_min, spec_max = spectrogram.min(), spectrogram.max()\n",
    "        if spec_max > spec_min: spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram_3ch = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "        return torch.tensor(spectrogram_3ch, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Prepare Data ---\n",
    "print(\"Loading pre-defined and balanced data splits...\")\n",
    "with open(os.path.join(FILE_LIST_PATH, 'train_files_unbalanced.pkl'), 'rb') as f: train_files_raw = pickle.load(f)\n",
    "with open(os.path.join(FILE_LIST_PATH, 'val_files_unbalanced.pkl'), 'rb') as f: val_files_raw = pickle.load(f)\n",
    "with open(os.path.join(FILE_LIST_PATH, 'test_files_unbalanced.pkl'), 'rb') as f: test_files_raw = pickle.load(f)\n",
    "\n",
    "print(\"Verifying that all spectrogram files exist...\")\n",
    "def verify_and_filter_files(file_list_raw):\n",
    "    verified_files = []\n",
    "    for f_path in file_list_raw:\n",
    "        # Use our new robust function here as well\n",
    "        npy_filename = get_basename(f_path).replace('.wav', '.npy')\n",
    "        full_npy_path = os.path.join(SPECTROGRAM_PATH, npy_filename)\n",
    "        if os.path.exists(full_npy_path):\n",
    "            # We keep the original path in the list for the label getter\n",
    "            verified_files.append(f_path)\n",
    "    skipped_count = len(file_list_raw) - len(verified_files)\n",
    "    return verified_files, skipped_count\n",
    "\n",
    "train_files, train_skipped = verify_and_filter_files(train_files_raw)\n",
    "val_files, val_skipped = verify_and_filter_files(val_files_raw)\n",
    "test_files, test_skipped = verify_and_filter_files(test_files_raw)\n",
    "\n",
    "print(f\"Train set: {len(train_files)} files found, {train_skipped} skipped.\")\n",
    "print(f\"Validation set: {len(val_files)} files found, {val_skipped} skipped.\")\n",
    "print(f\"Test set: {len(test_files)} files found, {test_skipped} skipped.\")\n",
    "\n",
    "# Create label lists\n",
    "ravdess_map = { \"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\" }\n",
    "crema_d_map = { \"NEU\": \"neutral\", \"HAP\": \"happy\", \"SAD\": \"sad\", \"ANG\": \"angry\", \"FEA\": \"fearful\", \"DIS\": \"disgust\" }\n",
    "def get_label_from_path(filepath):\n",
    "    filename = get_basename(filepath)\n",
    "    try:\n",
    "        if '03-01' in filename: return unified_emotion_map[ravdess_map[filename.split(\"-\")[2]]]\n",
    "        else: return unified_emotion_map[crema_d_map[filename.split(\"_\")[2]]]\n",
    "    except (IndexError, KeyError): return None\n",
    "\n",
    "train_labels = [get_label_from_path(f) for f in train_files]; val_labels = [get_label_from_path(f) for f in val_files]; test_labels = [get_label_from_path(f) for f in test_files]\n",
    "\n",
    "train_dataset = SpectrogramDataset(train_files, train_labels); val_dataset = SpectrogramDataset(val_files, val_labels); test_dataset = SpectrogramDataset(test_files, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2); val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2); test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- Train the Model ---\n",
    "model = models.resnet18(weights='IMAGENET1K_V1'); model.fc = nn.Linear(model.fc.in_features, len(unified_emotion_labels)); model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE); criterion = nn.CrossEntropyLoss()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "best_val_acc = 0.0\n",
    "print(\"Starting advanced training with SpecAugment...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(); running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = spec_augment_transform(inputs)\n",
    "        optimizer.zero_grad(); outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "        loss.backward(); optimizer.step(); running_loss += loss.item() * inputs.size(0)\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    model.eval(); val_loss = 0.0; correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs); loss = criterion(outputs, labels); val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1); total += labels.size(0); correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total; val_loss /= len(val_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        print(f\"🎉 New best validation accuracy: {best_val_acc:.2f}%. Saving model...\")\n",
    "        torch.save({'model_state_dict': model.state_dict()}, CHECKPOINT_BEST_PATH)\n",
    "    scheduler.step()\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "print(\"\\n--- FINAL EVALUATION OF ADVANCED GENERALIST MODEL ---\")\n",
    "print(f\"Loading best model (from epoch with {best_val_acc:.2f}% validation accuracy) for final testing...\")\n",
    "best_checkpoint = torch.load(CHECKPOINT_BEST_PATH); model.load_state_dict(best_checkpoint['model_state_dict']); model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs); _, preds = torch.max(outputs, 1); all_preds.extend(preds.cpu().numpy()); all_true.extend(labels.cpu().numpy())\n",
    "accuracy = accuracy_score(all_true, all_preds)\n",
    "print(f\"\\nFinal Advanced Generalist Model Accuracy on the Test Set: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\"); print(classification_report(all_true, all_preds, target_names=unified_emotion_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Verdict - Comparing Balanced vs. Unbalanced\n",
    "\n",
    "Here, we evaluate the model trained on unbalanced data. We test its performance on the RAVDESS and CREMA-D domains separately. The key goal is to compare these results directly against the champion v5 model's scores to quantify the impact of data balancing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5842,
     "status": "ok",
     "timestamp": 1756979644229,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "Ywtt38Q2DJaU",
    "outputId": "8f149661-a6ca-4a90-bc2a-f727d01a7800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading the best 'Ultimate Generalist' model...\n",
      "Loading the test set data split...\n",
      "File paths normalized for Linux environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on RAVDESS Test Set: 100%|██████████| 3/3 [00:00<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Accuracy on RAVDESS Test Set: 82.48%\n",
      "Classification Report for RAVDESS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.84      0.94      0.89        17\n",
      "       happy       0.83      0.89      0.86        27\n",
      "         sad       0.79      0.73      0.76        26\n",
      "       angry       0.87      0.72      0.79        18\n",
      "     fearful       0.83      0.86      0.84        22\n",
      "     disgust       0.81      0.81      0.81        27\n",
      "\n",
      "    accuracy                           0.82       137\n",
      "   macro avg       0.83      0.83      0.83       137\n",
      "weighted avg       0.82      0.82      0.82       137\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on CREMA-D Test Set: 100%|██████████| 18/18 [00:04<00:00,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Accuracy on CREMA-D Test Set: 64.85%\n",
      "Classification Report for CREMA-D Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.69      0.76      0.72       160\n",
      "       happy       0.64      0.66      0.65       187\n",
      "         sad       0.63      0.56      0.59       210\n",
      "       angry       0.71      0.77      0.74       199\n",
      "     fearful       0.63      0.56      0.59       193\n",
      "     disgust       0.58      0.61      0.60       189\n",
      "\n",
      "    accuracy                           0.65      1138\n",
      "   macro avg       0.65      0.65      0.65      1138\n",
      "weighted avg       0.65      0.65      0.65      1138\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# FINAL SCRIPT v2: Evaluating the Generalist Model (with Path Fix)\n",
    "# ===================================================================\n",
    "import torch, torch.nn as nn, os, numpy as np, pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "\n",
    "# --- Configuration ---\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms_final/\"\n",
    "FILE_LIST_PATH = \"/content/drive/MyDrive/ser_project/\"\n",
    "CHECKPOINT_BEST_PATH = \"/content/drive/MyDrive/ser_project/resnet_advanced_best.pth\"\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Mappings and Dataset Class ---\n",
    "unified_emotion_map = { \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3, \"fearful\": 4, \"disgust\": 5 }\n",
    "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
    "\n",
    "def get_basename(path): # Robust way to get filename\n",
    "    return path.replace('\\\\', '/').split('/')[-1]\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300):\n",
    "        self.file_paths, self.labels, self.target_width = file_paths, labels, target_width\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        filename = get_basename(self.file_paths[idx]).replace('.wav', '.npy')\n",
    "        file_path = os.path.join(SPECTROGRAM_PATH, filename)\n",
    "        label = self.labels[idx]\n",
    "        spectrogram = np.load(file_path)\n",
    "        current_width = spectrogram.shape[1]\n",
    "        if current_width < self.target_width: spectrogram = np.pad(spectrogram, ((0, 0), (0, self.target_width - current_width)), mode='constant')\n",
    "        elif current_width > self.target_width: spectrogram = spectrogram[:, :self.target_width]\n",
    "        spec_min, spec_max = spectrogram.min(), spectrogram.max()\n",
    "        if spec_max > spec_min: spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram_3ch = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "        return torch.tensor(spectrogram_3ch, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Load the Best Trained Model ---\n",
    "print(\"Loading the best 'Ultimate Generalist' model...\")\n",
    "model = models.resnet18(); model.fc = nn.Linear(model.fc.in_features, len(unified_emotion_labels));\n",
    "best_checkpoint = torch.load(CHECKPOINT_BEST_PATH); model.load_state_dict(best_checkpoint['model_state_dict']);\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Load the test set file list ---\n",
    "print(\"Loading the test set data split...\")\n",
    "with open(os.path.join(FILE_LIST_PATH, 'test_files_unbalanced.pkl'), 'rb') as f: test_files_raw = pickle.load(f)\n",
    "\n",
    "# --- THIS IS THE CRUCIAL FIX ---\n",
    "# Normalize Windows paths ('\\') to Linux paths ('/')\n",
    "test_files = [p.replace('\\\\', '/') for p in test_files_raw]\n",
    "print(\"File paths normalized for Linux environment.\")\n",
    "\n",
    "# --- Create the label list for the test set ---\n",
    "ravdess_map = { \"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\" }\n",
    "crema_d_map = { \"NEU\": \"neutral\", \"HAP\": \"happy\", \"SAD\": \"sad\", \"ANG\": \"angry\", \"FEA\": \"fearful\", \"DIS\": \"disgust\" }\n",
    "def get_label(filepath):\n",
    "    filename = get_basename(filepath)\n",
    "    try:\n",
    "        if '03-01' in filename: return unified_emotion_map[ravdess_map[filename.split(\"-\")[2]]]\n",
    "        else: return unified_emotion_map[crema_d_map[filename.split(\"_\")[2]]]\n",
    "    except (IndexError, KeyError): return None\n",
    "test_labels = [get_label(f) for f in test_files]\n",
    "\n",
    "# Filter out any files that might have failed label parsing\n",
    "valid_indices = [i for i, lbl in enumerate(test_labels) if lbl is not None]\n",
    "test_files = [test_files[i] for i in valid_indices]\n",
    "test_labels = [test_labels[i] for i in valid_indices]\n",
    "\n",
    "# --- Filter the test set for each dataset ---\n",
    "ravdess_test_files = [f for f in test_files if 'ravdess_data' in f.lower()]\n",
    "ravdess_test_labels = [l for i, l in enumerate(test_labels) if 'ravdess_data' in test_files[i].lower()]\n",
    "\n",
    "crema_d_test_files = [f for f in test_files if 'crema_d_data' in f.lower()]\n",
    "crema_d_test_labels = [l for i, l in enumerate(test_labels) if 'crema_d_data' in test_files[i].lower()]\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(files, labels, name):\n",
    "    dataset = SpectrogramDataset(files, labels)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labs in tqdm(loader, desc=f\"Evaluating on {name}\"):\n",
    "            inputs, labs = inputs.to(device), labs.to(device)\n",
    "            outputs = model(inputs); _, preds = torch.max(outputs, 1); all_preds.extend(preds.cpu().numpy()); all_true.extend(labs.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_true, all_preds)\n",
    "    print(f\"\\n>>> Accuracy on {name}: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Classification Report for {name}:\"); print(classification_report(all_true, all_preds, target_names=unified_emotion_labels, zero_division=0))\n",
    "\n",
    "# --- Run the Final Evaluations ---\n",
    "if ravdess_test_files:\n",
    "    evaluate(ravdess_test_files, ravdess_test_labels, \"RAVDESS Test Set\")\n",
    "if crema_d_test_files:\n",
    "    evaluate(crema_d_test_files, crema_d_test_labels, \"CREMA-D Test Set\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPiXvBoqx0fN56+9hfNv8Z/",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
