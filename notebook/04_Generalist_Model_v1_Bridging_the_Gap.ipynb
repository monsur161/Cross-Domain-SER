{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.3: The First Generalist Model - Bridging the Domain Gap\n",
    "\n",
    "**Objective:** After identifying the domain gap in Phase 2, the goal of this experiment is to solve it. \n",
    "\n",
    "Our strategy is to create a **\"Generalist Model\"** by training a single ResNet18 model on a combined, mixed dataset containing both the clean speech from **RAVDESS** and the more realistic speech from **CREMA-D**. The hypothesis is that this will force the model to learn more robust, universal features of emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36987,
     "status": "ok",
     "timestamp": 1756891248018,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "GJ1dN3K7sOqk",
    "outputId": "6969be3b-7917-41d4-f488-608d9dadca37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install only what's needed for training (no librosa/audio libs required here)\n",
    "!pip install pandas seaborn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Unified Data Strategy\n",
    "\n",
    "The core of this experiment is a new data strategy. First, we **harmonize the emotion labels** into a unified set of 6 core emotions that are common to both datasets. \n",
    "\n",
    "Next, we **merge the pre-computed spectrograms** from both RAVDESS and CREMA-D into a single large data pool. The model will be trained on a shuffled mix of this data, making it \"domain-agnostic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1699505,
     "status": "ok",
     "timestamp": 1756892957028,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "QCPLnfB9s4kn",
    "outputId": "7a7c7df1-d9e2-4fe3-d0d6-c566fa4d0ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing data from pre-computed spectrograms...\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 190MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [04:19<00:00,  2.29s/it]\n",
      "Epoch 1/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:06<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 1.4187 | Val Loss: 1.5573 | Val Acc: 36.94%\n",
      "ðŸŽ‰ New best validation accuracy: 36.94%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.69it/s]\n",
      "Epoch 2/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 1.2295 | Val Loss: 3.0299 | Val Acc: 23.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.75it/s]\n",
      "Epoch 3/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 1.1472 | Val Loss: 1.3326 | Val Acc: 45.49%\n",
      "ðŸŽ‰ New best validation accuracy: 45.49%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.64it/s]\n",
      "Epoch 4/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 1.0291 | Val Loss: 2.4633 | Val Acc: 32.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.75it/s]\n",
      "Epoch 5/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.9244 | Val Loss: 1.7096 | Val Acc: 41.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:40<00:00,  2.77it/s]\n",
      "Epoch 6/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.8146 | Val Loss: 1.4930 | Val Acc: 48.00%\n",
      "ðŸŽ‰ New best validation accuracy: 48.00%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.66it/s]\n",
      "Epoch 7/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.7022 | Val Loss: 1.5719 | Val Acc: 45.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:40<00:00,  2.76it/s]\n",
      "Epoch 8/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.4847 | Val Loss: 1.1349 | Val Acc: 59.37%\n",
      "ðŸŽ‰ New best validation accuracy: 59.37%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.66it/s]\n",
      "Epoch 9/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.3819 | Val Loss: 1.2359 | Val Acc: 58.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.75it/s]\n",
      "Epoch 10/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.3353 | Val Loss: 1.3499 | Val Acc: 56.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.76it/s]\n",
      "Epoch 11/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.3072 | Val Loss: 1.3866 | Val Acc: 57.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.73it/s]\n",
      "Epoch 12/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.2900 | Val Loss: 1.4753 | Val Acc: 56.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.73it/s]\n",
      "Epoch 13/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.2698 | Val Loss: 1.5088 | Val Acc: 56.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.71it/s]\n",
      "Epoch 14/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 0.2486 | Val Loss: 1.5624 | Val Acc: 56.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.68it/s]\n",
      "Epoch 15/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 0.2217 | Val Loss: 1.5697 | Val Acc: 57.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.71it/s]\n",
      "Epoch 16/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 0.2169 | Val Loss: 1.5843 | Val Acc: 55.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.70it/s]\n",
      "Epoch 17/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 0.2132 | Val Loss: 1.5766 | Val Acc: 56.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.70it/s]\n",
      "Epoch 18/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 0.2078 | Val Loss: 1.5968 | Val Acc: 56.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.71it/s]\n",
      "Epoch 19/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 0.2035 | Val Loss: 1.6206 | Val Acc: 55.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.68it/s]\n",
      "Epoch 20/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 0.1986 | Val Loss: 1.6114 | Val Acc: 56.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.71it/s]\n",
      "Epoch 21/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 0.1925 | Val Loss: 1.6395 | Val Acc: 56.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.71it/s]\n",
      "Epoch 22/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 0.1873 | Val Loss: 1.6574 | Val Acc: 56.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.68it/s]\n",
      "Epoch 23/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 0.1860 | Val Loss: 1.6531 | Val Acc: 55.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.66it/s]\n",
      "Epoch 24/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 0.1872 | Val Loss: 1.6723 | Val Acc: 56.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.70it/s]\n",
      "Epoch 25/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 0.1853 | Val Loss: 1.6476 | Val Acc: 56.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.70it/s]\n",
      "Epoch 26/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 0.1850 | Val Loss: 1.6505 | Val Acc: 56.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.68it/s]\n",
      "Epoch 27/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Loss: 0.1844 | Val Loss: 1.6622 | Val Acc: 56.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.69it/s]\n",
      "Epoch 28/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | Train Loss: 0.1841 | Val Loss: 1.6464 | Val Acc: 56.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:42<00:00,  2.69it/s]\n",
      "Epoch 29/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | Train Loss: 0.1843 | Val Loss: 1.6671 | Val Acc: 55.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 113/113 [00:41<00:00,  2.70it/s]\n",
      "Epoch 30/30 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | Train Loss: 0.1834 | Val Loss: 1.6559 | Val Acc: 55.84%\n",
      "\n",
      "--- FINAL EVALUATION ---\n",
      "Loading best model (from epoch with 59.37% validation accuracy) for final testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:05<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Generalist Model Accuracy on the Test Set: 59.37%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.71      0.60      0.65       178\n",
      "       happy       0.64      0.58      0.61       220\n",
      "         sad       0.41      0.68      0.51       219\n",
      "       angry       0.71      0.61      0.66       219\n",
      "     fearful       0.58      0.52      0.55       220\n",
      "     disgust       0.69      0.58      0.63       219\n",
      "\n",
      "    accuracy                           0.59      1275\n",
      "   macro avg       0.62      0.59      0.60      1275\n",
      "weighted avg       0.62      0.59      0.60      1275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import models\n",
    "\n",
    "# --- Configuration ---\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms/\"\n",
    "LEARNING_RATE = 0.001; BATCH_SIZE = 64; EPOCHS = 30\n",
    "CHECKPOINT_BEST_PATH = \"/content/drive/MyDrive/ser_project/resnet_generalist_best_v2.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Mappings ---\n",
    "unified_emotion_map = { \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3, \"fearful\": 4, \"disgust\": 5 }\n",
    "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
    "ravdess_map = { \"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\" }\n",
    "crema_d_map = { \"NEU\": \"neutral\", \"HAP\": \"happy\", \"SAD\": \"sad\", \"ANG\": \"angry\", \"FEA\": \"fearful\", \"DIS\": \"disgust\" }\n",
    "\n",
    "# --- A simpler and faster Dataset class for pre-computed files ---\n",
    "class PrecomputedSpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300):\n",
    "        self.file_paths, self.labels, self.target_width = file_paths, labels, target_width\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.file_paths[idx], self.labels[idx]\n",
    "        spectrogram = np.load(file_path)\n",
    "        if spectrogram.shape[1] < self.target_width: spectrogram = np.pad(spectrogram, ((0, 0), (0, self.target_width - spectrogram.shape[1])), mode='constant')\n",
    "        else: spectrogram = spectrogram[:, :self.target_width]\n",
    "        spec_min, spec_max = spectrogram.min(), spectrogram.max()\n",
    "        if spec_max > spec_min: spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram_3ch = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "        return torch.tensor(spectrogram_3ch, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Prepare Data from pre-computed .npy files ---\n",
    "print(\"Preparing data from pre-computed spectrograms...\")\n",
    "all_files = [os.path.join(SPECTROGRAM_PATH, f) for f in os.listdir(SPECTROGRAM_PATH) if f.endswith('.npy')]\n",
    "all_labels_str = []\n",
    "# This loop is designed to handle both RAVDESS and CREMA-D filenames\n",
    "for f in all_files:\n",
    "    filename = os.path.basename(f)\n",
    "    try:\n",
    "        if '03-01' in filename: # Heuristic for RAVDESS\n",
    "            code = filename.split(\"-\")[2]\n",
    "            if code in ravdess_map: all_labels_str.append(ravdess_map[code])\n",
    "        else: # Assumed to be CREMA-D\n",
    "            code = filename.split(\"_\")[2]\n",
    "            if code in crema_d_map: all_labels_str.append(crema_d_map[code])\n",
    "    except IndexError:\n",
    "        # print(f\"Could not parse filename: {filename}\")\n",
    "        continue\n",
    "# Filter out files that couldn't be parsed\n",
    "valid_indices = [i for i, lbl in enumerate(all_labels_str) if lbl]\n",
    "all_files = [all_files[i] for i in valid_indices]\n",
    "\n",
    "emotion_to_idx = {e: i for i, e in enumerate(unified_emotion_labels)}; all_labels = [emotion_to_idx[lbl] for lbl in all_labels_str]\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(all_files, all_labels, test_size=0.15, random_state=42, stratify=all_labels)\n",
    "train_dataset = PrecomputedSpectrogramDataset(train_files, train_labels); val_dataset = PrecomputedSpectrogramDataset(val_files, val_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0); val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- Train the Model (with Early Stopping) ---\n",
    "model = models.resnet18(weights='IMAGENET1K_V1'); model.fc = nn.Linear(model.fc.in_features, len(unified_emotion_labels)); model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE); criterion = nn.CrossEntropyLoss(); scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(); running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad(); outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "        loss.backward(); optimizer.step(); running_loss += loss.item() * inputs.size(0)\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    model.eval(); val_loss = 0.0; correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs); loss = criterion(outputs, labels); val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1); total += labels.size(0); correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total; val_loss /= len(val_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        print(f\"ðŸŽ‰ New best validation accuracy: {best_val_acc:.2f}%. Saving model...\")\n",
    "        torch.save({'model_state_dict': model.state_dict()}, CHECKPOINT_BEST_PATH)\n",
    "    scheduler.step()\n",
    "\n",
    "# --- Final Evaluation (using the held-out portion of the validation set as a test set) ---\n",
    "print(\"\\n--- FINAL EVALUATION ---\")\n",
    "# The val_files/val_labels serve as our final test set in this simplified script\n",
    "test_loader_final = val_loader\n",
    "print(f\"Loading best model (from epoch with {best_val_acc:.2f}% validation accuracy) for final testing...\")\n",
    "best_checkpoint = torch.load(CHECKPOINT_BEST_PATH); model.load_state_dict(best_checkpoint['model_state_dict']); model.eval()\n",
    "all_preds = []; all_true = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader_final, desc=\"Final Evaluation\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs); _, preds = torch.max(outputs, 1); all_preds.extend(preds.cpu().numpy()); all_true.extend(labels.cpu().numpy())\n",
    "accuracy = accuracy_score(all_true, all_preds)\n",
    "print(f\"\\nFinal Generalist Model Accuracy on the Test Set: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\"); print(classification_report(all_true, all_preds, target_names=unified_emotion_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Verdict - Evaluating the Generalist\n",
    "\n",
    "This final step is the ultimate test of our strategy. We take the single Generalist Model trained on the mixed data and evaluate its performance on the **RAVDESS and CREMA-D test sets separately**. This allows for a direct comparison against the failed Specialist Model from Phase 2, providing a definitive measure of whether the domain gap has been bridged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92350,
     "status": "ok",
     "timestamp": 1756893419504,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "TbhkfIMKAJox",
    "outputId": "843c678d-e7b5-4ec5-878e-7fd998af23cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading the best generalist model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on CREMA-D Test Set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:31<00:00,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Accuracy on CREMA-D Test Set: 64.61%\n",
      "Classification Report for CREMA-D Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.71      0.68      0.69       110\n",
      "       happy       0.69      0.66      0.68       127\n",
      "         sad       0.48      0.63      0.55       131\n",
      "       angry       0.78      0.69      0.73       132\n",
      "     fearful       0.59      0.56      0.57       133\n",
      "     disgust       0.71      0.66      0.69       130\n",
      "\n",
      "    accuracy                           0.65       763\n",
      "   macro avg       0.66      0.65      0.65       763\n",
      "weighted avg       0.66      0.65      0.65       763\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# FINAL SCRIPT: Evaluating the Generalist Model on Separate Domains\n",
    "# ===================================================================\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "\n",
    "# --- Configuration ---\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms/\"\n",
    "CHECKPOINT_BEST_PATH = \"/content/drive/MyDrive/ser_project/resnet_generalist_best_v2.pth\"\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Mappings and Dataset Class (same as before) ---\n",
    "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
    "class PrecomputedSpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300): #... (rest of the class is the same)\n",
    "        self.file_paths, self.labels, self.target_width = file_paths, labels, target_width\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.file_paths[idx], self.labels[idx]\n",
    "        spectrogram = np.load(file_path)\n",
    "        if spectrogram.shape[1] < self.target_width: spectrogram = np.pad(spectrogram, ((0, 0), (0, self.target_width - spectrogram.shape[1])), mode='constant')\n",
    "        else: spectrogram = spectrogram[:, :self.target_width]\n",
    "        spec_min, spec_max = spectrogram.min(), spectrogram.max()\n",
    "        if spec_max > spec_min: spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram_3ch = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "        return torch.tensor(spectrogram_3ch, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Load the Best Generalist Model ---\n",
    "print(\"Loading the best generalist model...\")\n",
    "model = models.resnet18(); model.fc = nn.Linear(model.fc.in_features, len(unified_emotion_labels));\n",
    "best_checkpoint = torch.load(CHECKPOINT_BEST_PATH); model.load_state_dict(best_checkpoint['model_state_dict']);\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Prepare the separate Test Sets ---\n",
    "# We need to recreate the exact same split to get our test set\n",
    "all_files = [os.path.join(SPECTROGRAM_PATH, f) for f in os.listdir(SPECTROGRAM_PATH) if f.endswith('.npy')]\n",
    "all_labels_str = []\n",
    "ravdess_map = { \"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\" }\n",
    "crema_d_map = { \"NEU\": \"neutral\", \"HAP\": \"happy\", \"SAD\": \"sad\", \"ANG\": \"angry\", \"FEA\": \"fearful\", \"DIS\": \"disgust\" }\n",
    "for f in all_files:\n",
    "    filename = os.path.basename(f)\n",
    "    try:\n",
    "        if '03-01' in filename:\n",
    "            code = filename.split(\"-\")[2]\n",
    "            if code in ravdess_map: all_labels_str.append(ravdess_map[code])\n",
    "        else:\n",
    "            code = filename.split(\"_\")[2]\n",
    "            if code in crema_d_map: all_labels_str.append(crema_d_map[code])\n",
    "    except IndexError: continue\n",
    "valid_indices = [i for i, lbl in enumerate(all_labels_str) if lbl]\n",
    "all_files = [all_files[i] for i in valid_indices]\n",
    "emotion_to_idx = {e: i for i, e in enumerate(unified_emotion_labels)}; all_labels = [emotion_to_idx[lbl] for lbl in all_labels_str]\n",
    "_, test_files, _, test_labels = train_test_split(all_files, all_labels, test_size=0.1, random_state=42, stratify=all_labels)\n",
    "\n",
    "# Filter the test set for each dataset\n",
    "ravdess_test_files = [f for f in test_files if 'RAVDESS' in f.upper()]\n",
    "ravdess_test_labels = [l for i, l in enumerate(test_labels) if 'RAVDESS' in test_files[i].upper()]\n",
    "crema_d_test_files = [f for f in test_files if 'CREMA-D' in f.upper() or '10' in os.path.basename(f)] # Heuristic for CREMA-D files\n",
    "crema_d_test_labels = [l for i, l in enumerate(test_labels) if 'CREMA-D' in test_files[i].upper() or '10' in os.path.basename(test_files[i])]\n",
    "\n",
    "# --- Run the Evaluations ---\n",
    "def evaluate(files, labels, name):\n",
    "    dataset = PrecomputedSpectrogramDataset(files, labels)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labs in tqdm(loader, desc=f\"Evaluating on {name}\"):\n",
    "            inputs, labs = inputs.to(device), labs.to(device)\n",
    "            outputs = model(inputs); _, preds = torch.max(outputs, 1); all_preds.extend(preds.cpu().numpy()); all_true.extend(labs.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_true, all_preds)\n",
    "    print(f\"\\n>>> Accuracy on {name}: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Classification Report for {name}:\"); print(classification_report(all_true, all_preds, target_names=unified_emotion_labels, zero_division=0))\n",
    "\n",
    "if ravdess_test_files: evaluate(ravdess_test_files, ravdess_test_labels, \"RAVDESS Test Set\")\n",
    "if crema_d_test_files: evaluate(crema_d_test_files, crema_d_test_labels, \"CREMA-D Test Set\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPbRRgbs8t3wIgmdvJT5325",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
