{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39348524c6fb4d43985729f166a1c46e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99b027d242564ebba9cb5d5bc9499198",
              "IPY_MODEL_c0122bb293bc40df9e7e61d6e2aa665a",
              "IPY_MODEL_348a5e1c8e924c19b96451289f116d77"
            ],
            "layout": "IPY_MODEL_cd3283d4f3774489baf5c51f3cb2ab3b"
          }
        },
        "99b027d242564ebba9cb5d5bc9499198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01e01c2dc9c9469686cc83632d3c3e71",
            "placeholder": "​",
            "style": "IPY_MODEL_ae287e43aaa24768a30581c32ba8ad0c",
            "value": "Fetching 1 files: 100%"
          }
        },
        "c0122bb293bc40df9e7e61d6e2aa665a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5036556a474141c4ac9345a5e1b57c73",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f694daa5d7fc4be3b33994ba28d5a203",
            "value": 1
          }
        },
        "348a5e1c8e924c19b96451289f116d77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e61da6b0059c469c9ef5efcf4c7d64e1",
            "placeholder": "​",
            "style": "IPY_MODEL_1deb39bac9904b909157298ebc056769",
            "value": " 1/1 [00:00&lt;00:00,  8.48it/s]"
          }
        },
        "cd3283d4f3774489baf5c51f3cb2ab3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01e01c2dc9c9469686cc83632d3c3e71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae287e43aaa24768a30581c32ba8ad0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5036556a474141c4ac9345a5e1b57c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f694daa5d7fc4be3b33994ba28d5a203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e61da6b0059c469c9ef5efcf4c7d64e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1deb39bac9904b909157298ebc056769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4818839e5f9b473296d233b8558dd2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd97fb95683a45f08482e3ffb0ef68d0",
              "IPY_MODEL_533ad67a6ed7402ba6df097be7d79c17",
              "IPY_MODEL_73934fc06af142878b65c7220b42d0b2"
            ],
            "layout": "IPY_MODEL_6d6034cedbaf408788f1f8def7d4a3e7"
          }
        },
        "cd97fb95683a45f08482e3ffb0ef68d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83ecabfe381f443280ff32f63ab82062",
            "placeholder": "​",
            "style": "IPY_MODEL_0c91afcc74f7461cbad5b4a6f7f33a15",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "533ad67a6ed7402ba6df097be7d79c17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b8669a2582545ad9e732937dc5ff956",
            "max": 159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e56591808cae41969c1d0ddac2036a5d",
            "value": 159
          }
        },
        "73934fc06af142878b65c7220b42d0b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaa516b768504726bf53b31358106f61",
            "placeholder": "​",
            "style": "IPY_MODEL_44c720cdad57422fa46f06ea1aa14b9a",
            "value": " 159/159 [00:00&lt;00:00, 19.9kB/s]"
          }
        },
        "6d6034cedbaf408788f1f8def7d4a3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83ecabfe381f443280ff32f63ab82062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c91afcc74f7461cbad5b4a6f7f33a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b8669a2582545ad9e732937dc5ff956": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e56591808cae41969c1d0ddac2036a5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaa516b768504726bf53b31358106f61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44c720cdad57422fa46f06ea1aa14b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4429efe3e424f1bbda5840c9f9b1602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5e6d56260774b0ab583efb9854cf74a",
              "IPY_MODEL_4e52ebceda5e4ce9a80c84b92ce709c2",
              "IPY_MODEL_2d37b291ac73434b99709be48f622863"
            ],
            "layout": "IPY_MODEL_1a35db8658df4b889fc6d25ce9c5617d"
          }
        },
        "f5e6d56260774b0ab583efb9854cf74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16d962afd75a490590212a8266f4ad53",
            "placeholder": "​",
            "style": "IPY_MODEL_de74b31268364ed18c2ff8f0a3b01dcb",
            "value": "config.json: "
          }
        },
        "4e52ebceda5e4ce9a80c84b92ce709c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f31e222a7d6d442c848e55628fdefd81",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7e93a2ff8e046d782e06c8bba4f6c55",
            "value": 1
          }
        },
        "2d37b291ac73434b99709be48f622863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_904fef74d60b4c08ab8dced2d8578bdf",
            "placeholder": "​",
            "style": "IPY_MODEL_b93f15c85205481080314ed863f31358",
            "value": " 1.60k/? [00:00&lt;00:00, 176kB/s]"
          }
        },
        "1a35db8658df4b889fc6d25ce9c5617d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d962afd75a490590212a8266f4ad53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de74b31268364ed18c2ff8f0a3b01dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f31e222a7d6d442c848e55628fdefd81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e7e93a2ff8e046d782e06c8bba4f6c55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "904fef74d60b4c08ab8dced2d8578bdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b93f15c85205481080314ed863f31358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2565325cafb7490f898a55a3674e5dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_668df023112d4a7a9d6246a1441b8920",
              "IPY_MODEL_6c0aeb1944e844b0ad692a4bc4c6862a",
              "IPY_MODEL_83ce2e7853114c6eb6854a656a05a071"
            ],
            "layout": "IPY_MODEL_25d46aefd3b2496f8dfcacbe155f9913"
          }
        },
        "668df023112d4a7a9d6246a1441b8920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf5a3a26f0c24a7bba68d9fc06b6a317",
            "placeholder": "​",
            "style": "IPY_MODEL_90213c1032804723b7a7c51c3e943fc3",
            "value": "model.safetensors: 100%"
          }
        },
        "6c0aeb1944e844b0ad692a4bc4c6862a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_066c0baf9d89441d92c59a5890f3dda3",
            "max": 377607901,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_753469fb86124851a00ea2425db9fe5d",
            "value": 377607901
          }
        },
        "83ce2e7853114c6eb6854a656a05a071": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37f63c0f1a714bda84f3c71a98140518",
            "placeholder": "​",
            "style": "IPY_MODEL_7de5c309ecb5488ebd15c68d21c93510",
            "value": " 378M/378M [00:01&lt;00:00, 566MB/s]"
          }
        },
        "25d46aefd3b2496f8dfcacbe155f9913": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf5a3a26f0c24a7bba68d9fc06b6a317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90213c1032804723b7a7c51c3e943fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "066c0baf9d89441d92c59a5890f3dda3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "753469fb86124851a00ea2425db9fe5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37f63c0f1a714bda84f3c71a98140518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de5c309ecb5488ebd15c68d21c93510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 4.8: The Paradigm Shift - Attempting a Wav2Vec2 Generalist\n",
        "\n",
        "**Objective:** This notebook documents the project's most significant methodological leap: moving from a CNN-on-spectrograms approach to a state-of-the-art, end-to-end Speech Transformer.\n",
        "\n",
        "The goal was to train a **Wav2Vec2** model to see if a native speech architecture could outperform our champion CNN. This involved three major upgrades:\n",
        "1.  **Expanding the Dataset** to include the naturalistic IEMOCAP dataset.\n",
        "2.  Building a **new data pipeline** for raw audio waveforms.\n",
        "3.  Implementing a **two-stage curriculum learning** strategy.\n",
        "\n",
        "**Outcome:** While the pipeline was successfully built, the model training failed due to numerical instability, providing a critical learning opportunity for the final, successful attempt."
      ],
      "metadata": {
        "id": "5oQpXBlOHP-n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5QtNdd8yB0l",
        "outputId": "73e841ce-10a6-4c34-c504-9795465e5087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting audiomentations\n",
            "  Downloading audiomentations-0.42.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.56.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (1.10.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
            "  Downloading numpy_minmax-0.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
            "  Downloading numpy_rms-0.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting python-stretch<1,>=0.3.1 (from audiomentations)\n",
            "  Downloading python_stretch-0.3.1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.9)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.2)\n",
            "Downloading audiomentations-0.42.0-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.5/86.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy_minmax-0.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading numpy_rms-0.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18 kB)\n",
            "Downloading python_stretch-0.3.1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m795.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-stretch, numpy-rms, numpy-minmax, audiomentations\n",
            "Successfully installed audiomentations-0.42.0 numpy-minmax-0.5.0 numpy-rms-0.6.0 python-stretch-0.3.1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install transformers[torch] datasets librosa pandas seaborn matplotlib tqdm audiomentations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Creating the Super-Dataset\n",
        "\n",
        "To train our most powerful model yet, we expand our data pool. In addition to RAVDESS and CREMA-D, we now incorporate the **IEMOCAP** dataset, which contains spontaneous, natural speech from dialogue scenarios. This creates a large and highly diverse dataset of over 11,000 samples, providing a robust testbed for our models."
      ],
      "metadata": {
        "id": "6PvZ3GLZHX2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 2: DATA PREPARATION\n",
        "# ===================================================================\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "\n",
        "# --- Configuration ---\n",
        "RAVDESS_PATH = \"/content/drive/MyDrive/ser_project/ravdess_data/\"\n",
        "CREMA_D_PATH = \"/content/drive/MyDrive/ser_project/crema_d_data/AudioWAV/\"\n",
        "IEMOCAP_PATH = \"/content/drive/MyDrive/ser_project/iemocap_data/IEMOCAP_full_release/\"\n",
        "\n",
        "# --- Mappings (6 core emotions) ---\n",
        "unified_emotion_map = { \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3, \"fearful\": 4, \"disgust\": 5 }\n",
        "ravdess_map = { \"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\" }\n",
        "crema_d_map = { \"NEU\": \"neutral\", \"HAP\": \"happy\", \"SAD\": \"sad\", \"ANG\": \"angry\", \"FEA\": \"fearful\", \"DIS\": \"disgust\" }\n",
        "iemocap_map = { \"neu\": \"neutral\", \"hap\": \"happy\", \"sad\": \"sad\", \"ang\": \"angry\", \"fea\": \"fearful\", \"exc\": \"happy\" } # Map excited to happy\n",
        "\n",
        "# --- Gather files and labels from all three datasets ---\n",
        "all_files = []\n",
        "all_labels_str = []\n",
        "print(\"--- GATHERING AND COUNTING FILES ---\")\n",
        "\n",
        "# Process RAVDESS\n",
        "ravdess_count = 0\n",
        "for root, dirs, files in os.walk(RAVDESS_PATH):\n",
        "    for f in files:\n",
        "        if f.endswith('.wav'):\n",
        "            try:\n",
        "                code = f.split(\"-\")[2]\n",
        "                if code in ravdess_map:\n",
        "                    all_files.append(os.path.join(root, f))\n",
        "                    all_labels_str.append(ravdess_map[code])\n",
        "                    ravdess_count += 1\n",
        "            except IndexError:\n",
        "                continue\n",
        "print(f\"Found {ravdess_count} relevant files in RAVDESS.\")\n",
        "\n",
        "# Process CREMA-D\n",
        "crema_d_count = 0\n",
        "if os.path.exists(CREMA_D_PATH):\n",
        "    for f in os.listdir(CREMA_D_PATH):\n",
        "        if f.endswith('.wav'):\n",
        "            try:\n",
        "                code = f.split(\"_\")[2]\n",
        "                if code in crema_d_map:\n",
        "                    all_files.append(os.path.join(CREMA_D_PATH, f))\n",
        "                    all_labels_str.append(crema_d_map[code])\n",
        "                    crema_d_count += 1\n",
        "            except IndexError:\n",
        "                continue\n",
        "print(f\"Found {crema_d_count} relevant files in CREMA-D.\")\n",
        "\n",
        "# Process IEMOCAP\n",
        "iemocap_count = 0\n",
        "if os.path.exists(IEMOCAP_PATH):\n",
        "    for session_folder in os.listdir(IEMOCAP_PATH):\n",
        "        if session_folder.startswith(\"Session\"):\n",
        "            emo_path = os.path.join(IEMOCAP_PATH, session_folder, \"dialog/EmoEvaluation/\")\n",
        "            wav_root = os.path.join(IEMOCAP_PATH, session_folder, \"sentences/wav/\")\n",
        "            if os.path.isdir(emo_path) and os.path.isdir(wav_root):\n",
        "                for txt_file in os.listdir(emo_path):\n",
        "                    if txt_file.endswith('.txt'):\n",
        "                        with open(os.path.join(emo_path, txt_file)) as f_ann:\n",
        "                            for line in f_ann:\n",
        "                                if line.startswith('['):\n",
        "                                    parts = line.strip().split('\\t')\n",
        "                                    if len(parts) >= 3 and parts[2] in iemocap_map:\n",
        "                                        wav_folder = parts[1].rsplit('_', 1)[0]\n",
        "                                        wav_file = os.path.join(wav_root, wav_folder, f\"{parts[1]}.wav\")\n",
        "                                        if os.path.exists(wav_file):\n",
        "                                            all_files.append(wav_file)\n",
        "                                            all_labels_str.append(iemocap_map[parts[2]])\n",
        "                                            iemocap_count += 1\n",
        "print(f\"Found {iemocap_count} relevant files in IEMOCAP.\")\n",
        "print(f\"\\nTotal files found across all datasets: {len(all_files)}\")\n",
        "\n",
        "# --- Create final data splits ---\n",
        "# 80% train, 10% validation, 10% test\n",
        "train_val_files, test_files, train_val_labels_str, test_labels_str = train_test_split(\n",
        "    all_files, all_labels_str, test_size=0.15, random_state=42, stratify=all_labels_str\n",
        ")\n",
        "train_files, val_files, train_labels_str, val_labels_str = train_test_split(\n",
        "    train_val_files, train_val_labels_str, test_size=0.1, random_state=42, stratify=train_val_labels_str\n",
        ")\n",
        "\n",
        "print(\"\\n--- DATA SPLITTING COMPLETE ---\")\n",
        "print(f\"Training samples: {len(train_files)}\")\n",
        "print(f\"Validation samples: {len(val_files)}\")\n",
        "print(f\"Test samples: {len(test_files)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeMZBB9Dy_aL",
        "outputId": "bbfede62-fefb-40cb-88c7-e9a0412ba151"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GATHERING AND COUNTING FILES ---\n",
            "Found 1056 relevant files in RAVDESS.\n",
            "Found 7442 relevant files in CREMA-D.\n",
            "Found 3438 relevant files in IEMOCAP.\n",
            "\n",
            "Total files found across all datasets: 11936\n",
            "\n",
            "--- DATA SPLITTING COMPLETE ---\n",
            "Training samples: 9130\n",
            "Validation samples: 1015\n",
            "Test samples: 1791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: A New Pipeline for Raw Audio\n",
        "\n",
        "Unlike our previous CNNs that required spectrogram \"images,\" Transformers like Wav2Vec2 can process raw audio waveforms directly. This requires a completely new data pipeline.\n",
        "\n",
        "We define a `WavDataset` that loads audio and resamples it to the required 16kHz. A custom `collate_fn` then uses the official Hugging Face `Wav2Vec2FeatureExtractor` to pad batches and convert them into the format the model expects."
      ],
      "metadata": {
        "id": "iJmytJs_Hc5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 3: HELPER DEFINITIONS\n",
        "# ===================================================================\n",
        "import torch\n",
        "import librosa\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Wav2Vec2FeatureExtractor\n",
        "\n",
        "# --- Initialize Feature Extractor (used by the collate function) ---\n",
        "model_name = \"facebook/wav2vec2-base-960h\"\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "# --- Wav2Vec2 Dataset Class ---\n",
        "class WavDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load audio at the required 16kHz sample rate\n",
        "        speech_array, sr = librosa.load(self.file_paths[idx], sr=16000)\n",
        "        return speech_array, self.labels[idx]\n",
        "\n",
        "# --- Collate Function to process batches ---\n",
        "def collate_fn(batch):\n",
        "    features, labels = zip(*batch)\n",
        "    # The feature_extractor handles padding and tensor conversion\n",
        "    processed = feature_extractor(list(features), sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
        "    return processed['input_values'], torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "print(\"✅ Helper classes and functions are defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222,
          "referenced_widgets": [
            "39348524c6fb4d43985729f166a1c46e",
            "99b027d242564ebba9cb5d5bc9499198",
            "c0122bb293bc40df9e7e61d6e2aa665a",
            "348a5e1c8e924c19b96451289f116d77",
            "cd3283d4f3774489baf5c51f3cb2ab3b",
            "01e01c2dc9c9469686cc83632d3c3e71",
            "ae287e43aaa24768a30581c32ba8ad0c",
            "5036556a474141c4ac9345a5e1b57c73",
            "f694daa5d7fc4be3b33994ba28d5a203",
            "e61da6b0059c469c9ef5efcf4c7d64e1",
            "1deb39bac9904b909157298ebc056769",
            "4818839e5f9b473296d233b8558dd2ba",
            "cd97fb95683a45f08482e3ffb0ef68d0",
            "533ad67a6ed7402ba6df097be7d79c17",
            "73934fc06af142878b65c7220b42d0b2",
            "6d6034cedbaf408788f1f8def7d4a3e7",
            "83ecabfe381f443280ff32f63ab82062",
            "0c91afcc74f7461cbad5b4a6f7f33a15",
            "3b8669a2582545ad9e732937dc5ff956",
            "e56591808cae41969c1d0ddac2036a5d",
            "aaa516b768504726bf53b31358106f61",
            "44c720cdad57422fa46f06ea1aa14b9a"
          ]
        },
        "id": "-51Od1muzpOo",
        "outputId": "6005f849-5bc4-400c-cfcd-81ec74c02677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39348524c6fb4d43985729f166a1c46e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4818839e5f9b473296d233b8558dd2ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Helper classes and functions are defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: The Training Failure - `NaN` Loss\n",
        "\n",
        "Our training plan was a two-stage curriculum. **Stage 1**, documented in this cell, aimed to train an \"Acted Speech Expert\" on the combined RAVDESS and CREMA-D datasets.\n",
        "\n",
        "However, the training was unsuccessful. The log shows the training loss immediately becoming `NaN` (Not a Number), which indicates a numerical instability issue (like exploding gradients). As a result, the model could not learn, and its validation accuracy remained at the level of random chance. The subsequent Stage 2 (adapting to IEMOCAP) also failed as a consequence."
      ],
      "metadata": {
        "id": "Dja4-iKsHjNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 4: STAGE 1 - TRAINING THE ACTED SPEECH EXPERT\n",
        "# ===================================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import Wav2Vec2ForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration for Stage 1 ---\n",
        "LEARNING_RATE = 3e-5\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 15\n",
        "CHECKPOINT_STAGE1_PATH = \"/content/drive/MyDrive/ser_project/wav2vec2_stage1_acted_best.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
        "emotion_to_idx = {e: i for i, e in enumerate(unified_emotion_labels)}\n",
        "\n",
        "# --- Prepare RAVDESS + CREMA-D data splits ---\n",
        "# These variables (train_files, etc.) must be available from Cell 2\n",
        "acted_train_files = [f for f in train_files if 'iemocap_data' not in f]\n",
        "acted_val_files = [f for f in val_files if 'iemocap_data' not in f]\n",
        "acted_train_labels = [emotion_to_idx[lbl] for i, lbl in enumerate(train_labels_str) if 'iemocap_data' not in train_files[i]]\n",
        "acted_val_labels = [emotion_to_idx[lbl] for i, lbl in enumerate(val_labels_str) if 'iemocap_data' not in val_files[i]]\n",
        "\n",
        "# The WavDataset and collate_fn must be available from Cell 3\n",
        "train_dataset = WavDataset(acted_train_files, acted_train_labels)\n",
        "val_dataset = WavDataset(acted_val_files, acted_val_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
        "print(f\"Starting Stage 1: Training on {len(train_dataset)} acted samples...\")\n",
        "\n",
        "# --- Initialize Model, Optimizer, Scheduler, and Scaler ---\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=len(unified_emotion_labels))\n",
        "model.freeze_feature_extractor = False\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_training_steps = len(train_loader) * EPOCHS\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# --- Stage 1 Training Loop with AMP ---\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Stage 1 - Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=f\"Stage 1 - Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs).logits\n",
        "                loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_loss /= len(val_dataset)\n",
        "    print(f\"Stage 1 - Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    if val_accuracy > best_val_acc:\n",
        "        best_val_acc = val_accuracy\n",
        "        print(f\"🎉 New best Stage 1 validation accuracy: {best_val_acc:.2f}%. Saving model...\")\n",
        "        torch.save({'model_state_dict': model.state_dict()}, CHECKPOINT_STAGE1_PATH)\n",
        "\n",
        "print(\"\\n✅ Stage 1 training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c4429efe3e424f1bbda5840c9f9b1602",
            "f5e6d56260774b0ab583efb9854cf74a",
            "4e52ebceda5e4ce9a80c84b92ce709c2",
            "2d37b291ac73434b99709be48f622863",
            "1a35db8658df4b889fc6d25ce9c5617d",
            "16d962afd75a490590212a8266f4ad53",
            "de74b31268364ed18c2ff8f0a3b01dcb",
            "f31e222a7d6d442c848e55628fdefd81",
            "e7e93a2ff8e046d782e06c8bba4f6c55",
            "904fef74d60b4c08ab8dced2d8578bdf",
            "b93f15c85205481080314ed863f31358",
            "2565325cafb7490f898a55a3674e5dd1",
            "668df023112d4a7a9d6246a1441b8920",
            "6c0aeb1944e844b0ad692a4bc4c6862a",
            "83ce2e7853114c6eb6854a656a05a071",
            "25d46aefd3b2496f8dfcacbe155f9913",
            "cf5a3a26f0c24a7bba68d9fc06b6a317",
            "90213c1032804723b7a7c51c3e943fc3",
            "066c0baf9d89441d92c59a5890f3dda3",
            "753469fb86124851a00ea2425db9fe5d",
            "37f63c0f1a714bda84f3c71a98140518",
            "7de5c309ecb5488ebd15c68d21c93510"
          ]
        },
        "id": "eKmqJvtYzsdy",
        "outputId": "3c88cd57-461c-4030-87eb-8229610c448a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Stage 1: Training on 6498 acted samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4429efe3e424f1bbda5840c9f9b1602"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2565325cafb7490f898a55a3674e5dd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3354382679.py:44: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "Stage 1 - Epoch 1/15:   0%|          | 0/813 [00:00<?, ?it/s]/tmp/ipython-input-3354382679.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Stage 1 - Epoch 1/15: 100%|██████████| 813/813 [15:20<00:00,  1.13s/it]\n",
            "Stage 1 - Epoch 1/15 [Val]:   0%|          | 0/91 [00:00<?, ?it/s]/tmp/ipython-input-3354382679.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Stage 1 - Epoch 1/15 [Val]: 100%|██████████| 91/91 [01:43<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 1/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n",
            "🎉 New best Stage 1 validation accuracy: 19.06%. Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 2/15: 100%|██████████| 813/813 [00:46<00:00, 17.41it/s]\n",
            "Stage 1 - Epoch 2/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 27.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 2/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 3/15: 100%|██████████| 813/813 [00:46<00:00, 17.52it/s]\n",
            "Stage 1 - Epoch 3/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 27.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 3/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 4/15: 100%|██████████| 813/813 [00:45<00:00, 17.71it/s]\n",
            "Stage 1 - Epoch 4/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 27.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 4/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 5/15: 100%|██████████| 813/813 [00:45<00:00, 17.83it/s]\n",
            "Stage 1 - Epoch 5/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 26.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 5/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 6/15: 100%|██████████| 813/813 [00:46<00:00, 17.54it/s]\n",
            "Stage 1 - Epoch 6/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 26.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 6/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 7/15: 100%|██████████| 813/813 [00:45<00:00, 17.75it/s]\n",
            "Stage 1 - Epoch 7/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 26.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 7/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 8/15: 100%|██████████| 813/813 [00:47<00:00, 17.20it/s]\n",
            "Stage 1 - Epoch 8/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 26.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 8/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 9/15: 100%|██████████| 813/813 [00:45<00:00, 17.75it/s]\n",
            "Stage 1 - Epoch 9/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 25.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 9/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 10/15: 100%|██████████| 813/813 [00:46<00:00, 17.61it/s]\n",
            "Stage 1 - Epoch 10/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 25.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 10/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 11/15: 100%|██████████| 813/813 [00:46<00:00, 17.48it/s]\n",
            "Stage 1 - Epoch 11/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 25.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 11/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 12/15: 100%|██████████| 813/813 [00:45<00:00, 17.73it/s]\n",
            "Stage 1 - Epoch 12/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 26.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 12/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 13/15: 100%|██████████| 813/813 [00:45<00:00, 17.75it/s]\n",
            "Stage 1 - Epoch 13/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 26.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 13/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 14/15: 100%|██████████| 813/813 [00:46<00:00, 17.58it/s]\n",
            "Stage 1 - Epoch 14/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 27.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 14/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 1 - Epoch 15/15: 100%|██████████| 813/813 [00:46<00:00, 17.34it/s]\n",
            "Stage 1 - Epoch 15/15 [Val]: 100%|██████████| 91/91 [00:03<00:00, 26.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1 - Epoch 15/15 | Train Loss: nan | Val Loss: 1.7905 | Val Acc: 19.06%\n",
            "\n",
            "✅ Stage 1 training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 5: STAGE 2 - ADAPTING TO NATURAL SPEECH (IEMOCAP)\n",
        "# ===================================================================\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import Wav2Vec2ForSequenceClassification, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration for Stage 2 ---\n",
        "LEARNING_RATE = 1e-5 # Use a smaller LR for the second, more delicate fine-tuning stage\n",
        "EPOCHS = 20\n",
        "CHECKPOINT_STAGE1_PATH = \"/content/drive/MyDrive/ser_project/wav2vec2_stage1_acted_best.pth\"\n",
        "CHECKPOINT_STAGE2_PATH = \"/content/drive/MyDrive/ser_project/wav2vec2_stage2_final_best.pth\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");\n",
        "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
        "emotion_to_idx = {e: i for i, e in enumerate(unified_emotion_labels)}\n",
        "\n",
        "# --- Prepare IEMOCAP-only data splits ---\n",
        "# These variables (train_files, etc.) must be available from Cell 2\n",
        "iemocap_train_files = [f for f in train_files if 'iemocap_data' in f]\n",
        "iemocap_val_files = [f for f in val_files if 'iemocap_data' in f]\n",
        "iemocap_train_labels = [emotion_to_idx[lbl] for i, lbl in enumerate(train_labels_str) if 'iemocap_data' in train_files[i]]\n",
        "iemocap_val_labels = [emotion_to_idx[lbl] for i, lbl in enumerate(val_labels_str) if 'iemocap_data' in val_files[i]]\n",
        "\n",
        "# The WavDataset and collate_fn must be available from Cell 3\n",
        "train_dataset = WavDataset(iemocap_train_files, iemocap_train_labels)\n",
        "val_dataset = WavDataset(iemocap_val_files, iemocap_val_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "print(f\"Starting Stage 2: Adapting on {len(train_dataset)} natural IEMOCAP samples...\")\n",
        "\n",
        "# --- Load the Stage 1 Model ---\n",
        "print(\"Loading Stage 1 model (Acted Speech Expert)...\")\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"facebook/wav2vec2-base-960h\", num_labels=len(unified_emotion_labels))\n",
        "model.freeze_feature_extractor = False\n",
        "stage1_checkpoint = torch.load(CHECKPOINT_STAGE1_PATH)\n",
        "model.load_state_dict(stage1_checkpoint['model_state_dict'])\n",
        "model = model.to(device)\n",
        "\n",
        "# --- Initialize a new Optimizer, Scheduler, and Scaler for Stage 2 ---\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_training_steps = len(train_loader) * EPOCHS\n",
        "num_warmup_steps = int(0.1 * num_training_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# --- Stage 2 Training Loop with AMP ---\n",
        "best_val_acc = 0.0\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train(); running_loss = 0.0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Stage 2 - Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs).logits\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    train_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval(); val_loss = 0.0; correct = 0; total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=f\"Stage 2 - Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(inputs).logits\n",
        "                loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1); total += labels.size(0); correct += (predicted == labels).sum().item()\n",
        "    val_accuracy = 100 * correct / total; val_loss /= len(val_dataset)\n",
        "    print(f\"Stage 2 - Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "    if val_accuracy > best_val_acc:\n",
        "        best_val_acc = val_accuracy\n",
        "        print(f\"🎉 New best Stage 2 validation accuracy: {best_val_acc:.2f}%. Saving final model...\")\n",
        "        torch.save({'model_state_dict': model.state_dict()}, CHECKPOINT_STAGE2_PATH)\n",
        "\n",
        "print(\"\\n✅ Stage 2 training complete. The final model is saved.\")"
      ],
      "metadata": {
        "id": "VFV6tfkkz4Zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b0d54da-0cbe-4bd0-b5ee-c52be9d8b3bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Stage 2: Adapting on 2632 natural IEMOCAP samples...\n",
            "Loading Stage 1 model (Acted Speech Expert)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-3812416904.py:48: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "Stage 2 - Epoch 1/20:   0%|          | 0/329 [00:00<?, ?it/s]/tmp/ipython-input-3812416904.py:56: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Stage 2 - Epoch 1/20: 100%|██████████| 329/329 [12:30<00:00,  2.28s/it]\n",
            "Stage 2 - Epoch 1/20 [Val]:   0%|          | 0/37 [00:00<?, ?it/s]/tmp/ipython-input-3812416904.py:74: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Stage 2 - Epoch 1/20 [Val]: 100%|██████████| 37/37 [01:18<00:00,  2.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 1/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n",
            "🎉 New best Stage 2 validation accuracy: 15.12%. Saving final model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 2/20: 100%|██████████| 329/329 [00:36<00:00,  8.97it/s]\n",
            "Stage 2 - Epoch 2/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 20.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 2/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 3/20: 100%|██████████| 329/329 [00:34<00:00,  9.44it/s]\n",
            "Stage 2 - Epoch 3/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 3/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 4/20: 100%|██████████| 329/329 [00:33<00:00,  9.72it/s]\n",
            "Stage 2 - Epoch 4/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 4/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 5/20: 100%|██████████| 329/329 [00:33<00:00,  9.78it/s]\n",
            "Stage 2 - Epoch 5/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 20.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 5/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 6/20: 100%|██████████| 329/329 [00:33<00:00,  9.91it/s]\n",
            "Stage 2 - Epoch 6/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 6/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 7/20: 100%|██████████| 329/329 [00:33<00:00,  9.96it/s]\n",
            "Stage 2 - Epoch 7/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 20.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 7/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 8/20: 100%|██████████| 329/329 [00:33<00:00,  9.96it/s]\n",
            "Stage 2 - Epoch 8/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 8/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 9/20: 100%|██████████| 329/329 [00:33<00:00,  9.85it/s]\n",
            "Stage 2 - Epoch 9/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 20.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 9/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 10/20: 100%|██████████| 329/329 [00:33<00:00,  9.92it/s]\n",
            "Stage 2 - Epoch 10/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 10/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 11/20: 100%|██████████| 329/329 [00:33<00:00,  9.91it/s]\n",
            "Stage 2 - Epoch 11/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 11/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 12/20: 100%|██████████| 329/329 [00:33<00:00,  9.95it/s]\n",
            "Stage 2 - Epoch 12/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 12/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 13/20: 100%|██████████| 329/329 [00:32<00:00, 10.02it/s]\n",
            "Stage 2 - Epoch 13/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 13/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 14/20: 100%|██████████| 329/329 [00:32<00:00,  9.97it/s]\n",
            "Stage 2 - Epoch 14/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 20.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 14/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 15/20: 100%|██████████| 329/329 [00:33<00:00,  9.94it/s]\n",
            "Stage 2 - Epoch 15/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 15/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 16/20: 100%|██████████| 329/329 [00:32<00:00,  9.98it/s]\n",
            "Stage 2 - Epoch 16/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 16/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 17/20: 100%|██████████| 329/329 [00:32<00:00, 10.01it/s]\n",
            "Stage 2 - Epoch 17/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 20.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 17/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 18/20: 100%|██████████| 329/329 [00:32<00:00, 10.04it/s]\n",
            "Stage 2 - Epoch 18/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 20.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 18/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 19/20: 100%|██████████| 329/329 [00:32<00:00, 10.04it/s]\n",
            "Stage 2 - Epoch 19/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 19/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage 2 - Epoch 20/20: 100%|██████████| 329/329 [00:33<00:00,  9.91it/s]\n",
            "Stage 2 - Epoch 20/20 [Val]: 100%|██████████| 37/37 [00:01<00:00, 19.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 2 - Epoch 20/20 | Train Loss: nan | Val Loss: 1.7929 | Val Acc: 15.12%\n",
            "\n",
            "✅ Stage 2 training complete. The final model is saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}