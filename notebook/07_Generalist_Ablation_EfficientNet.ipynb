{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.7: Control Experiment - Pushing the Limits with EfficientNet\n",
    "\n",
    "**Objective:** This notebook represents the final experiment within our CNN-on-spectrograms paradigm. Having determined that data balancing is crucial, we now investigate the impact of the model architecture itself.\n",
    "\n",
    "Our goal is to test if a more modern and powerful CNN, **EfficientNet-B2**, can outperform our previous ResNet18 model on the same unbalanced dataset, thereby establishing the performance ceiling for this entire approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36074,
     "status": "ok",
     "timestamp": 1757103063862,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "AjKA7esZelb3",
    "outputId": "ac76ed0c-93df-4806-8114-5d8b519175ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 1: SETUP\n",
    "# ===================================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Install all necessary packages for the entire notebook\n",
    "!pip install librosa pandas seaborn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40540,
     "status": "ok",
     "timestamp": 1757103110222,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "KelWe7RaevhN",
    "outputId": "9b45c16e-5d03-4f1e-b68f-6e7ec927a8df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GATHERING UNBALANCED FILES ---\n",
      "Found 1056 relevant files in RAVDESS.\n",
      "Found 7442 relevant files in CREMA-D.\n",
      "\n",
      "Total files found across both datasets: 8498\n",
      "\n",
      "--- DATA SPLITTING COMPLETE ---\n",
      "Training samples: 6500\n",
      "Validation samples: 723\n",
      "Test samples: 1275\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 2: DATA PREPARATION (UNBALANCED)\n",
    "# ===================================================================\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- Configuration ---\n",
    "RAVDESS_PATH = \"/content/drive/MyDrive/ser_project/ravdess_data/\"\n",
    "CREMA_D_PATH = \"/content/drive/MyDrive/ser_project/crema_d_data/AudioWAV/\"\n",
    "\n",
    "# --- Mappings (6 core emotions) ---\n",
    "unified_emotion_map = { \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3, \"fearful\": 4, \"disgust\": 5 }\n",
    "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
    "ravdess_map = { \"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\" }\n",
    "crema_d_map = { \"NEU\": \"neutral\", \"HAP\": \"happy\", \"SAD\": \"sad\", \"ANG\": \"angry\", \"FEA\": \"fearful\", \"DIS\": \"disgust\" }\n",
    "\n",
    "# --- Gather files and labels ---\n",
    "all_files = []\n",
    "all_labels_str = []\n",
    "print(\"--- GATHERING UNBALANCED FILES ---\")\n",
    "# Process RAVDESS\n",
    "ravdess_count = 0\n",
    "for root, dirs, files in os.walk(RAVDESS_PATH):\n",
    "    for f in files:\n",
    "        if f.endswith('.wav'):\n",
    "            try:\n",
    "                code = f.split(\"-\")[2]\n",
    "                if code in ravdess_map:\n",
    "                    all_files.append(os.path.join(root, f)); all_labels_str.append(ravdess_map[code])\n",
    "                    ravdess_count += 1\n",
    "            except IndexError: continue\n",
    "print(f\"Found {ravdess_count} relevant files in RAVDESS.\")\n",
    "# Process CREMA-D\n",
    "crema_d_count = 0\n",
    "for f in os.listdir(CREMA_D_PATH):\n",
    "    if f.endswith('.wav'):\n",
    "        try:\n",
    "            code = f.split(\"_\")[2]\n",
    "            if code in crema_d_map:\n",
    "                all_files.append(os.path.join(CREMA_D_PATH, f)); all_labels_str.append(crema_d_map[code])\n",
    "                crema_d_count += 1\n",
    "        except IndexError: continue\n",
    "print(f\"Found {crema_d_count} relevant files in CREMA-D.\")\n",
    "print(f\"\\nTotal files found across both datasets: {len(all_files)}\")\n",
    "\n",
    "# --- Create final data splits ---\n",
    "train_val_files, test_files, train_val_labels_str, test_labels_str = train_test_split(\n",
    "    all_files, all_labels_str, test_size=0.15, random_state=42, stratify=all_labels_str\n",
    ")\n",
    "train_files, val_files, train_labels_str, val_labels_str = train_test_split(\n",
    "    train_val_files, train_val_labels_str, test_size=0.1, random_state=42, stratify=train_val_labels_str\n",
    ")\n",
    "\n",
    "print(\"\\n--- DATA SPLITTING COMPLETE ---\")\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")\n",
    "print(f\"Test samples: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Upgrading the Architecture and Scheduler\n",
    "\n",
    "To create the strongest possible contender, we make two significant upgrades to our training process:\n",
    "\n",
    "* **New Architecture:** We replace the `ResNet18` backbone with `EfficientNet-B2`, a more modern architecture known for its high performance and parameter efficiency.\n",
    "* **New Scheduler:** We use the `OneCycleLR` scheduler, an aggressive and effective technique for achieving faster and more stable training convergence.\n",
    "\n",
    "To ensure a fair comparison with the ResNet18 model from the `v6` experiment, we deliberately use the same **unbalanced** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2790151,
     "status": "ok",
     "timestamp": 1757105912165,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "_3Xde6yxe2L0",
    "outputId": "4718bf5b-42d1-4932-88d2-dd6a3c49d2e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Initializing EfficientNet-B2 model...\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35.2M/35.2M [00:00<00:00, 194MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting final push training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 [Train]: 100%|██████████| 204/204 [32:20<00:00,  9.51s/it]\n",
      "Epoch 1/40 [Val]: 100%|██████████| 23/23 [03:33<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 1.7083 | Val Loss: 1.5713 | Val Acc: 36.93%\n",
      "🎉 New best validation accuracy: 36.93%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.44it/s]\n",
      "Epoch 2/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 | Train Loss: 1.4840 | Val Loss: 1.3698 | Val Acc: 42.88%\n",
      "🎉 New best validation accuracy: 42.88%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.76it/s]\n",
      "Epoch 3/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 | Train Loss: 1.2867 | Val Loss: 1.1761 | Val Acc: 54.77%\n",
      "🎉 New best validation accuracy: 54.77%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.91it/s]\n",
      "Epoch 4/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 | Train Loss: 1.1004 | Val Loss: 1.1523 | Val Acc: 56.02%\n",
      "🎉 New best validation accuracy: 56.02%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 [Train]: 100%|██████████| 204/204 [00:15<00:00, 13.60it/s]\n",
      "Epoch 5/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 | Train Loss: 0.9716 | Val Loss: 1.1306 | Val Acc: 56.98%\n",
      "🎉 New best validation accuracy: 56.98%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.61it/s]\n",
      "Epoch 6/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 | Train Loss: 0.8300 | Val Loss: 1.0954 | Val Acc: 60.44%\n",
      "🎉 New best validation accuracy: 60.44%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.84it/s]\n",
      "Epoch 7/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 | Train Loss: 0.7367 | Val Loss: 1.2087 | Val Acc: 56.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.42it/s]\n",
      "Epoch 8/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 | Train Loss: 0.6510 | Val Loss: 1.3307 | Val Acc: 56.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.78it/s]\n",
      "Epoch 9/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 | Train Loss: 0.5588 | Val Loss: 1.2706 | Val Acc: 59.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.11it/s]\n",
      "Epoch 10/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 | Train Loss: 0.4778 | Val Loss: 1.5273 | Val Acc: 58.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.01it/s]\n",
      "Epoch 11/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 | Train Loss: 0.3962 | Val Loss: 1.3352 | Val Acc: 61.27%\n",
      "🎉 New best validation accuracy: 61.27%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.05it/s]\n",
      "Epoch 12/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 | Train Loss: 0.3442 | Val Loss: 1.7864 | Val Acc: 55.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.45it/s]\n",
      "Epoch 13/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 | Train Loss: 0.3080 | Val Loss: 1.3761 | Val Acc: 59.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.39it/s]\n",
      "Epoch 14/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 | Train Loss: 0.2447 | Val Loss: 1.5179 | Val Acc: 61.69%\n",
      "🎉 New best validation accuracy: 61.69%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.69it/s]\n",
      "Epoch 15/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 | Train Loss: 0.2222 | Val Loss: 1.5635 | Val Acc: 62.10%\n",
      "🎉 New best validation accuracy: 62.10%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.75it/s]\n",
      "Epoch 16/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 | Train Loss: 0.2223 | Val Loss: 1.5120 | Val Acc: 60.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.06it/s]\n",
      "Epoch 17/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 | Train Loss: 0.2230 | Val Loss: 1.4848 | Val Acc: 63.07%\n",
      "🎉 New best validation accuracy: 63.07%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 [Train]: 100%|██████████| 204/204 [00:15<00:00, 13.55it/s]\n",
      "Epoch 18/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 | Train Loss: 0.1361 | Val Loss: 1.7777 | Val Acc: 60.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.20it/s]\n",
      "Epoch 19/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 | Train Loss: 0.1298 | Val Loss: 1.9125 | Val Acc: 61.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.13it/s]\n",
      "Epoch 20/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 | Train Loss: 0.1000 | Val Loss: 1.6218 | Val Acc: 64.45%\n",
      "🎉 New best validation accuracy: 64.45%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.64it/s]\n",
      "Epoch 21/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 | Train Loss: 0.1189 | Val Loss: 1.6954 | Val Acc: 63.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.06it/s]\n",
      "Epoch 22/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 | Train Loss: 0.0930 | Val Loss: 1.6291 | Val Acc: 62.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.97it/s]\n",
      "Epoch 23/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 | Train Loss: 0.0880 | Val Loss: 1.7166 | Val Acc: 62.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.40it/s]\n",
      "Epoch 24/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 | Train Loss: 0.0630 | Val Loss: 1.8655 | Val Acc: 62.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.40it/s]\n",
      "Epoch 25/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 | Train Loss: 0.0614 | Val Loss: 1.7133 | Val Acc: 64.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.25it/s]\n",
      "Epoch 26/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 | Train Loss: 0.0514 | Val Loss: 1.7796 | Val Acc: 61.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.20it/s]\n",
      "Epoch 27/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 | Train Loss: 0.0417 | Val Loss: 1.7562 | Val Acc: 63.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.36it/s]\n",
      "Epoch 28/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 | Train Loss: 0.0330 | Val Loss: 1.6261 | Val Acc: 65.84%\n",
      "🎉 New best validation accuracy: 65.84%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 13.82it/s]\n",
      "Epoch 29/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 | Train Loss: 0.0226 | Val Loss: 1.7424 | Val Acc: 63.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.31it/s]\n",
      "Epoch 30/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 | Train Loss: 0.0276 | Val Loss: 1.8138 | Val Acc: 64.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.09it/s]\n",
      "Epoch 31/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 | Train Loss: 0.0202 | Val Loss: 1.7802 | Val Acc: 64.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.29it/s]\n",
      "Epoch 32/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 | Train Loss: 0.0170 | Val Loss: 1.8110 | Val Acc: 64.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.19it/s]\n",
      "Epoch 33/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 | Train Loss: 0.0171 | Val Loss: 1.7431 | Val Acc: 65.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.35it/s]\n",
      "Epoch 34/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 | Train Loss: 0.0171 | Val Loss: 1.7832 | Val Acc: 64.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.39it/s]\n",
      "Epoch 35/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 | Train Loss: 0.0154 | Val Loss: 1.7493 | Val Acc: 64.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.26it/s]\n",
      "Epoch 36/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 | Train Loss: 0.0113 | Val Loss: 1.7236 | Val Acc: 64.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.41it/s]\n",
      "Epoch 37/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 | Train Loss: 0.0106 | Val Loss: 1.7442 | Val Acc: 65.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.46it/s]\n",
      "Epoch 38/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 16.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 | Train Loss: 0.0099 | Val Loss: 1.7554 | Val Acc: 62.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.16it/s]\n",
      "Epoch 39/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 14.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 | Train Loss: 0.0098 | Val Loss: 1.7880 | Val Acc: 64.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 [Train]: 100%|██████████| 204/204 [00:14<00:00, 14.47it/s]\n",
      "Epoch 40/40 [Val]: 100%|██████████| 23/23 [00:01<00:00, 15.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 | Train Loss: 0.0062 | Val Loss: 1.7633 | Val Acc: 64.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 3: THE FINAL PUSH - TRAINING WITH EFFICIENTNET & ONECYCLELR\n",
    "# ===================================================================\n",
    "import torch, torch.nn as nn, os, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Configuration ---\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms_final/\"\n",
    "MAX_LEARNING_RATE = 5e-4; BATCH_SIZE = 32; EPOCHS = 40\n",
    "CHECKPOINT_BEST_PATH = \"/content/drive/MyDrive/ser_project/efficientnet_final_push_best.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "emotion_to_idx = {e: i for i, e in enumerate(unified_emotion_labels)}\n",
    "\n",
    "# --- SpecAugment & Dataset Class ---\n",
    "spec_augment_transform = transforms.Compose([\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.05), ratio=(0.2, 5.0), value=0),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.08), ratio=(0.01, 0.2), value=0),\n",
    "])\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300):\n",
    "        self.file_paths, self.labels, self.target_width = file_paths, labels, target_width\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.path.basename(self.file_paths[idx]).replace('.wav', '.npy')\n",
    "        file_path = os.path.join(SPECTROGRAM_PATH, filename)\n",
    "        label = self.labels[idx]\n",
    "        spectrogram = np.load(file_path)\n",
    "        current_width = spectrogram.shape[1]\n",
    "        if current_width < self.target_width: spectrogram = np.pad(spectrogram, ((0, 0), (0, self.target_width - current_width)), mode='constant')\n",
    "        elif current_width > self.target_width: spectrogram = spectrogram[:, :self.target_width]\n",
    "        spec_min, spec_max = spectrogram.min(), spectrogram.max()\n",
    "        if spec_max > spec_min: spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram_3ch = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "        return torch.tensor(spectrogram_3ch, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Create Datasets & DataLoaders ---\n",
    "train_labels = [emotion_to_idx[lbl] for lbl in train_labels_str]; val_labels = [emotion_to_idx[lbl] for lbl in val_labels_str]\n",
    "train_dataset = SpectrogramDataset(train_files, train_labels); val_dataset = SpectrogramDataset(val_files, val_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2); val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- Initialize Model: EfficientNet-B2 ---\n",
    "print(\"Initializing EfficientNet-B2 model...\")\n",
    "model = models.efficientnet_b2(weights='IMAGENET1K_V1')\n",
    "num_ftrs = model.classifier[1].in_features; model.classifier[1] = nn.Linear(num_ftrs, len(unified_emotion_labels)); model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=MAX_LEARNING_RATE); criterion = nn.CrossEntropyLoss()\n",
    "scheduler = OneCycleLR(optimizer, max_lr=MAX_LEARNING_RATE, total_steps=len(train_loader) * EPOCHS)\n",
    "\n",
    "# --- Training Loop ---\n",
    "best_val_acc = 0.0\n",
    "print(\"Starting final push training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(); running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = spec_augment_transform(inputs)\n",
    "        optimizer.zero_grad(); outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "        loss.backward(); optimizer.step(); scheduler.step(); running_loss += loss.item() * inputs.size(0)\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    model.eval(); val_loss = 0.0; correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs); loss = criterion(outputs, labels); val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1); total += labels.size(0); correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total; val_loss /= len(val_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy; print(f\"🎉 New best validation accuracy: {best_val_acc:.2f}%. Saving model...\"); torch.save({'model_state_dict': model.state_dict()}, CHECKPOINT_BEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Verdict - ResNet18 vs. EfficientNet-B2\n",
    "\n",
    "This is the final comparison. We evaluate the newly trained EfficientNet model on the separate RAVDESS and CREMA-D test sets. The results are then compared directly against both our champion balanced model (v5) and the unbalanced ResNet18 model (v6) to draw a final conclusion on the effectiveness of the CNN-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 734766,
     "status": "ok",
     "timestamp": 1757106657298,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "zzZTZzgCe863",
    "outputId": "139db02b-0136-4361-d606-fd7593c029c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading best 'Final Push' model (EfficientNet-B2)...\n",
      "Preparing separate test sets for evaluation...\n",
      "\n",
      "--- FINAL DETAILED EVALUATION ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on RAVDESS Test Set: 100%|██████████| 5/5 [01:19<00:00, 15.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Accuracy on RAVDESS Test Set: 80.88%\n",
      "Classification Report for RAVDESS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.76      1.00      0.87        13\n",
      "       happy       0.71      0.88      0.79        25\n",
      "         sad       0.88      0.52      0.65        27\n",
      "       angry       0.87      0.83      0.85        24\n",
      "     fearful       0.76      0.96      0.85        23\n",
      "     disgust       0.95      0.79      0.86        24\n",
      "\n",
      "    accuracy                           0.81       136\n",
      "   macro avg       0.82      0.83      0.81       136\n",
      "weighted avg       0.83      0.81      0.80       136\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on CREMA-D Test Set: 100%|██████████| 36/36 [10:54<00:00, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Accuracy on CREMA-D Test Set: 64.97%\n",
      "Classification Report for CREMA-D Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.69      0.79      0.74       164\n",
      "       happy       0.63      0.58      0.60       195\n",
      "         sad       0.58      0.58      0.58       192\n",
      "       angry       0.70      0.82      0.76       196\n",
      "     fearful       0.61      0.54      0.58       197\n",
      "     disgust       0.67      0.61      0.64       195\n",
      "\n",
      "    accuracy                           0.65      1139\n",
      "   macro avg       0.65      0.65      0.65      1139\n",
      "weighted avg       0.65      0.65      0.65      1139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# CELL 4: FINAL DETAILED EVALUATION OF EFFICIENTNET MODEL\n",
    "# ===================================================================\n",
    "import torch, torch.nn as nn, os, numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "\n",
    "# --- Configuration ---\n",
    "CHECKPOINT_BEST_PATH = \"/content/drive/MyDrive/ser_project/efficientnet_final_push_best.pth\"\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms_final/\"\n",
    "BATCH_SIZE = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Mappings and Dataset Class (must be defined here to work) ---\n",
    "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
    "emotion_to_idx = {e: i for i, e in enumerate(unified_emotion_labels)}\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300):\n",
    "        self.file_paths, self.labels, self.target_width = file_paths, labels, target_width\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.path.basename(self.file_paths[idx]).replace('.wav', '.npy')\n",
    "        file_path = os.path.join(SPECTROGRAM_PATH, filename)\n",
    "        label = self.labels[idx]\n",
    "        spectrogram = np.load(file_path)\n",
    "        current_width = spectrogram.shape[1]\n",
    "        if current_width < self.target_width: spectrogram = np.pad(spectrogram, ((0, 0), (0, self.target_width - current_width)), mode='constant')\n",
    "        elif current_width > self.target_width: spectrogram = spectrogram[:, :self.target_width]\n",
    "        spec_min, spec_max = spectrogram.min(), spectrogram.max()\n",
    "        if spec_max > spec_min: spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram_3ch = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "        return torch.tensor(spectrogram_3ch, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Load the Best Trained Model ---\n",
    "print(\"Loading best 'Final Push' model (EfficientNet-B2)...\")\n",
    "model = models.efficientnet_b2()\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, len(unified_emotion_labels))\n",
    "best_checkpoint = torch.load(CHECKPOINT_BEST_PATH, map_location=device)\n",
    "model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Prepare the Separate Test Sets ---\n",
    "# Uses the 'test_files' and 'test_labels_str' variables from Cell 2\n",
    "print(\"Preparing separate test sets for evaluation...\")\n",
    "test_labels = [emotion_to_idx[lbl] for lbl in test_labels_str]\n",
    "\n",
    "# Filter for RAVDESS files\n",
    "ravdess_test_files = [f for f in test_files if 'ravdess_data' in f.lower().replace('\\\\', '/')]\n",
    "ravdess_test_labels = [l for i, l in enumerate(test_labels) if 'ravdess_data' in test_files[i].lower().replace('\\\\', '/')]\n",
    "\n",
    "# Filter for CREMA-D files\n",
    "crema_d_test_files = [f for f in test_files if 'crema_d_data' in f.lower().replace('\\\\', '/')]\n",
    "crema_d_test_labels = [l for i, l in enumerate(test_labels) if 'crema_d_data' in test_files[i].lower().replace('\\\\', '/')]\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(files, labels, name):\n",
    "    if not files:\n",
    "        print(f\"\\nSkipping evaluation for {name}: No test files found.\")\n",
    "        return\n",
    "\n",
    "    dataset = SpectrogramDataset(files, labels)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labs in tqdm(loader, desc=f\"Evaluating on {name}\"):\n",
    "            inputs, labs = inputs.to(device), labs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_true.extend(labs.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_true, all_preds)\n",
    "    print(f\"\\n>>> Accuracy on {name}: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(all_true, all_preds, target_names=unified_emotion_labels, zero_division=0))\n",
    "\n",
    "# --- Run the Final Evaluations ---\n",
    "print(\"\\n--- FINAL DETAILED EVALUATION ---\")\n",
    "if ravdess_test_files:\n",
    "    evaluate(ravdess_test_files, ravdess_test_labels, \"RAVDESS Test Set\")\n",
    "if crema_d_test_files:\n",
    "    evaluate(crema_d_test_files, crema_d_test_labels, \"CREMA-D Test Set\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP5sSyFKe62wq4Cy2fG4T4D",
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
