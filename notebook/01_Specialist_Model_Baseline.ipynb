{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Specialist Model Baseline\n",
    "\n",
    "**Objective:** The goal of this initial phase is to establish a strong performance baseline for Speech Emotion Recognition (SER). \n",
    "\n",
    "We will use a modern computer vision approach on a single, clean dataset (RAVDESS). This involves two main parts:\n",
    "1.  **Data Transformation:** Converting raw audio files into image-like Mel spectrograms.\n",
    "2.  **Modeling & Training:** Using a pre-trained Convolutional Neural Network (CNN) via Transfer Learning to build a \"Specialist Model\" that is an expert on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4624,
     "status": "ok",
     "timestamp": 1756674275214,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "IMHKsE93oOfu",
    "outputId": "8fb29785-a7b7-4908-9949-bd38b8ad7ccb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11183,
     "status": "ok",
     "timestamp": 1756674286403,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "DlpS1JjGuGw1",
    "outputId": "a26ad166-f507-4860-bc76-ffb9a916a4be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: resampy in /usr/local/lib/python3.12/dist-packages (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from resampy) (2.0.2)\n",
      "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.12/dist-packages (from resampy) (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.53->resampy) (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install resampy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation: From Audio to Images\n",
    "\n",
    "In this step, we process the entire RAVDESS dataset. Each `.wav` audio file is loaded, standardized to a 3-second clip, and then converted into a Log-Mel spectrogram. This transforms our audio problem into an image classification problem, allowing us to leverage powerful, pre-existing computer vision models. The final spectrograms are saved as `.npy` files for efficient loading during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140847,
     "status": "ok",
     "timestamp": 1756674427269,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "WDostxUJpKtj",
    "outputId": "d1860eb2-b839-4a66-a62d-b22399b08871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio to spectrogram conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Actors: 100%|██████████| 24/24 [02:20<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spectrogram conversion complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# Make sure these paths match your Google Drive structure\n",
    "AUDIO_PATH = \"/content/drive/MyDrive/ser_project/ravdess_data/\"\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/ravdess_spectrograms/\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(SPECTROGRAM_PATH, exist_ok=True)\n",
    "\n",
    "# --- Preprocessing Script ---\n",
    "print(\"Starting audio to spectrogram conversion...\")\n",
    "\n",
    "actor_folders = [f for f in os.listdir(AUDIO_PATH) if os.path.isdir(os.path.join(AUDIO_PATH, f))]\n",
    "for actor_folder in tqdm(actor_folders, desc=\"Processing Actors\"):\n",
    "    actor_path = os.path.join(AUDIO_PATH, actor_folder)\n",
    "    for file_name in os.listdir(actor_path):\n",
    "        try:\n",
    "            file_path = os.path.join(actor_path, file_name)\n",
    "\n",
    "            # Load audio\n",
    "            audio, sr = librosa.load(file_path, res_type='kaiser_fast', duration=3, sr=22050*2, offset=0.5)\n",
    "\n",
    "            # Generate Mel Spectrogram\n",
    "            spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128, fmax=8000)\n",
    "            db_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "\n",
    "            # Save the spectrogram as a NumPy array\n",
    "            output_filename = os.path.join(SPECTROGRAM_PATH, f\"{os.path.splitext(file_name)[0]}.npy\")\n",
    "            np.save(output_filename, db_spectrogram)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {file_path}: {e}\")\n",
    "\n",
    "print(\"\\nSpectrogram conversion complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2966,
     "status": "ok",
     "timestamp": 1756674430229,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "6vyDxC81zpbh",
    "outputId": "f85a3244-cb60-45c7-9f24-6af55619892b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Model class defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(AudioCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
    "\n",
    "        # Flattening and fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        # The input features to the linear layer will depend on the spectrogram size.\n",
    "        # We will calculate this dynamically later. For now, a placeholder.\n",
    "        self.fc1 = nn.Linear(in_features=64 * 8 * 8, out_features=128) # Placeholder size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add a channel dimension\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Conv blocks\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # Flatten and classify\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"CNN Model class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling, Training, and Evaluation\n",
    "\n",
    "With our data prepared, we now build and train our model. Our strategy is **Transfer Learning**. Instead of training a small CNN from scratch, we import a powerful **ResNet18** model that was pre-trained on the ImageNet dataset. We adapt this model for our task by replacing its final classification layer with one that matches our 8 emotion classes. The model is then trained on our spectrograms, and its performance is tracked using a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 300612,
     "status": "ok",
     "timestamp": 1756674730864,
     "user": {
      "displayName": "Monsur Abdullah",
      "userId": "12726162145146472590"
     },
     "user_tz": -360
    },
    "id": "w20NUpwWztrv",
    "outputId": "2bea1944-5957-4074-fef3-4187e6edec69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 80.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with ResNet18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 36/36 [00:21<00:00,  1.65it/s]\n",
      "Epoch 1/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | Train Loss: 1.4527 | Val Loss: 2.0887 | Val Acc: 45.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.43it/s]\n",
      "Epoch 2/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | Train Loss: 0.9335 | Val Loss: 1.5112 | Val Acc: 50.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.47it/s]\n",
      "Epoch 3/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | Train Loss: 0.7445 | Val Loss: 1.7743 | Val Acc: 45.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.41it/s]\n",
      "Epoch 4/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | Train Loss: 0.5086 | Val Loss: 1.5974 | Val Acc: 48.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.37it/s]\n",
      "Epoch 5/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | Train Loss: 0.4297 | Val Loss: 0.9689 | Val Acc: 67.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.49it/s]\n",
      "Epoch 6/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | Train Loss: 0.3207 | Val Loss: 2.3494 | Val Acc: 40.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.44it/s]\n",
      "Epoch 7/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | Train Loss: 0.2567 | Val Loss: 1.2624 | Val Acc: 68.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 36/36 [00:09<00:00,  4.00it/s]\n",
      "Epoch 8/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | Train Loss: 0.0959 | Val Loss: 0.5771 | Val Acc: 81.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.49it/s]\n",
      "Epoch 9/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | Train Loss: 0.0336 | Val Loss: 0.5338 | Val Acc: 83.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.49it/s]\n",
      "Epoch 10/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | Train Loss: 0.0256 | Val Loss: 0.5341 | Val Acc: 84.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.34it/s]\n",
      "Epoch 11/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | Train Loss: 0.0179 | Val Loss: 0.5262 | Val Acc: 84.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.40it/s]\n",
      "Epoch 12/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | Train Loss: 0.0161 | Val Loss: 0.5207 | Val Acc: 84.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 36/36 [00:07<00:00,  4.59it/s]\n",
      "Epoch 13/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | Train Loss: 0.0162 | Val Loss: 0.5304 | Val Acc: 84.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.45it/s]\n",
      "Epoch 14/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | Train Loss: 0.0151 | Val Loss: 0.5277 | Val Acc: 84.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████| 36/36 [00:07<00:00,  4.62it/s]\n",
      "Epoch 15/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | Train Loss: 0.0087 | Val Loss: 0.5226 | Val Acc: 85.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████| 36/36 [00:07<00:00,  4.51it/s]\n",
      "Epoch 16/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | Train Loss: 0.0084 | Val Loss: 0.5207 | Val Acc: 84.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.47it/s]\n",
      "Epoch 17/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | Train Loss: 0.0115 | Val Loss: 0.5289 | Val Acc: 85.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████| 36/36 [00:07<00:00,  4.55it/s]\n",
      "Epoch 18/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | Train Loss: 0.0110 | Val Loss: 0.5277 | Val Acc: 85.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|██████████| 36/36 [00:07<00:00,  4.51it/s]\n",
      "Epoch 19/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | Train Loss: 0.0079 | Val Loss: 0.5229 | Val Acc: 86.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.41it/s]\n",
      "Epoch 20/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | Train Loss: 0.0103 | Val Loss: 0.5284 | Val Acc: 84.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.44it/s]\n",
      "Epoch 21/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | Train Loss: 0.0074 | Val Loss: 0.5240 | Val Acc: 87.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|██████████| 36/36 [00:07<00:00,  4.52it/s]\n",
      "Epoch 22/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | Train Loss: 0.0078 | Val Loss: 0.5181 | Val Acc: 86.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.37it/s]\n",
      "Epoch 23/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | Train Loss: 0.0087 | Val Loss: 0.5214 | Val Acc: 86.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.39it/s]\n",
      "Epoch 24/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | Train Loss: 0.0095 | Val Loss: 0.5205 | Val Acc: 84.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.47it/s]\n",
      "Epoch 25/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | Train Loss: 0.0094 | Val Loss: 0.5245 | Val Acc: 85.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.44it/s]\n",
      "Epoch 26/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | Train Loss: 0.0083 | Val Loss: 0.5232 | Val Acc: 85.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.33it/s]\n",
      "Epoch 27/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  7.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | Train Loss: 0.0072 | Val Loss: 0.5216 | Val Acc: 86.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.26it/s]\n",
      "Epoch 28/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | Train Loss: 0.0093 | Val Loss: 0.5294 | Val Acc: 84.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.21it/s]\n",
      "Epoch 29/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | Train Loss: 0.0073 | Val Loss: 0.5210 | Val Acc: 86.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|██████████| 36/36 [00:08<00:00,  4.11it/s]\n",
      "Epoch 30/30 [Val]: 100%|██████████| 5/5 [00:00<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | Train Loss: 0.0102 | Val Loss: 0.5193 | Val Acc: 86.81%\n",
      "\n",
      "--- FINAL EVALUATION ---\n",
      "ResNet18 Model Accuracy on Test Set: 82.64%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.78      0.70      0.74        10\n",
      "        calm       0.70      0.84      0.76        19\n",
      "       happy       0.81      0.89      0.85        19\n",
      "         sad       0.79      0.79      0.79        19\n",
      "       angry       0.89      0.89      0.89        19\n",
      "     fearful       0.88      0.70      0.78        20\n",
      "     disgust       1.00      0.84      0.91        19\n",
      "    surprise       0.81      0.89      0.85        19\n",
      "\n",
      "    accuracy                           0.83       144\n",
      "   macro avg       0.83      0.82      0.82       144\n",
      "weighted avg       0.84      0.83      0.83       144\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# Import pre-trained models from torchvision\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F # Often needed, good to have\n",
    "\n",
    "# --- Configuration ---\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/ravdess_spectrograms/\"\n",
    "LEARNING_RATE = 0.001 # A good starting LR for fine-tuning\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30 # We often need fewer epochs when fine-tuning\n",
    "CHECKPOINT_PATH = \"/content/drive/MyDrive/ser_project/resnet_checkpoint.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "emotion_map = { \"01\": 0, \"02\": 1, \"03\": 2, \"04\": 3, \"05\": 4, \"06\": 5, \"07\": 6, \"08\": 7 }\n",
    "emotion_labels_list = [\"neutral\", \"calm\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\", \"surprise\"]\n",
    "\n",
    "# --- Custom PyTorch Dataset (Modified for 3-Channels) ---\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spectrogram = np.load(self.file_paths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Padding and Truncating\n",
    "        current_width = spectrogram.shape[1]\n",
    "        if current_width < self.target_width:\n",
    "            padding = self.target_width - current_width\n",
    "            spectrogram = np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n",
    "        elif current_width > self.target_width:\n",
    "            spectrogram = spectrogram[:, :self.target_width]\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        spec_min = spectrogram.min()\n",
    "        spec_max = spectrogram.max()\n",
    "        if spec_max > spec_min:\n",
    "            spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "\n",
    "        # Stack the single-channel spectrogram to create a 3-channel image for ResNet\n",
    "        spectrogram = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "\n",
    "        return torch.tensor(spectrogram, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Prepare Data ---\n",
    "all_files = [os.path.join(SPECTROGRAM_PATH, f) for f in os.listdir(SPECTROGRAM_PATH) if f.endswith('.npy')]\n",
    "all_labels = [emotion_map[os.path.basename(f).split(\"-\")[2]] for f in all_files]\n",
    "\n",
    "# 80% train, 10% validation, 10% test split\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "    all_files, all_labels, test_size=0.2, random_state=42, stratify=all_labels\n",
    ")\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "    temp_files, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n",
    "\n",
    "train_dataset = SpectrogramDataset(train_files, train_labels)\n",
    "val_dataset = SpectrogramDataset(val_files, val_labels)\n",
    "test_dataset = SpectrogramDataset(test_files, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Initialize Pre-trained ResNet18 Model ---\n",
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "\n",
    "# Adapt the final fully-connected layer for our 8 emotion classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(emotion_labels_list))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Optimizer, Scheduler, and Checkpoint Loading ---\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "start_epoch = 0\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(\"Checkpoint found! Loading model state...\")\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f\"Resuming training from epoch {start_epoch + 1}\")\n",
    "\n",
    "# --- Training Loop with Validation ---\n",
    "print(\"Starting training with ResNet18...\")\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    torch.save({ 'epoch': epoch + 1, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict() }, CHECKPOINT_PATH)\n",
    "\n",
    "# --- Final Evaluation on the TEST SET ---\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_true = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_true.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_true, all_preds)\n",
    "print(f\"\\n--- FINAL EVALUATION ---\")\n",
    "print(f\"ResNet18 Model Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_true, all_preds, target_names=emotion_labels_list, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNqN8c5K60BFViKnkaFiqLS",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
