{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4.5: The Ultimate Generalist - Achieving Peak Performance\n",
    "\n",
    "**Objective:** Building on the success of our first Generalist model, the goal of this phase is to create the \"Ultimate Generalist\" by incorporating several state-of-the-art techniques to maximize performance and robustness across both domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40667,
     "status": "ok",
     "timestamp": 1756962864564,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "OEMfPXsKASQT",
    "outputId": "d1233981-9a0a-4012-af90-7123544587e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Collecting audiomentations\n",
      "  Downloading audiomentations-0.42.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
      "Collecting numpy-minmax<1,>=0.3.0 (from audiomentations)\n",
      "  Downloading numpy_minmax-0.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting numpy-rms<1,>=0.4.2 (from audiomentations)\n",
      "  Downloading numpy_rms-0.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting python-stretch<1,>=0.3.1 (from audiomentations)\n",
      "  Downloading python_stretch-0.3.1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from numpy-minmax<1,>=0.3.0->audiomentations) (1.17.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.0->numpy-minmax<1,>=0.3.0->audiomentations) (2.22)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n",
      "Downloading audiomentations-0.42.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.5/86.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy_minmax-0.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading numpy_rms-0.6.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18 kB)\n",
      "Downloading python_stretch-0.3.1-cp312-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (109 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.4/109.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-stretch, numpy-rms, numpy-minmax, audiomentations\n",
      "Successfully installed audiomentations-0.42.0 numpy-minmax-0.5.0 numpy-rms-0.6.0 python-stretch-0.3.1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Connect to Google Drive to access your files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 2: Install all the necessary special libraries for our project\n",
    "# This single line installs everything we need.\n",
    "!pip install librosa audiomentations pandas seaborn matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33391,
     "status": "ok",
     "timestamp": 1756962897927,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "Tfi-4KyUIL02",
    "outputId": "f758127a-5dcb-4c9b-ea01-0f59405d90c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for folder at: /content/drive/MyDrive/ser_project/processed_spectrograms_final/\n",
      "\n",
      "✅ Success! Found the folder.\n",
      "Total files found in the folder: 8882\n",
      "\n",
      "Here are the first 10 filenames found:\n",
      "03-01-01-01-01-01-01.npy\n",
      "03-01-01-01-01-01-02.npy\n",
      "03-01-01-01-01-01-03.npy\n",
      "03-01-01-01-01-01-04.npy\n",
      "03-01-01-01-01-01-05.npy\n",
      "03-01-01-01-01-01-06.npy\n",
      "03-01-01-01-01-01-07.npy\n",
      "03-01-01-01-01-01-08.npy\n",
      "03-01-01-01-01-01-09.npy\n",
      "03-01-01-01-01-01-10.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The path we expect the files to be in\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms_final/\"\n",
    "\n",
    "print(f\"Checking for folder at: {SPECTROGRAM_PATH}\")\n",
    "\n",
    "try:\n",
    "    # Get a list of all files in that directory\n",
    "    all_npy_files = os.listdir(SPECTROGRAM_PATH)\n",
    "\n",
    "    print(f\"\\n✅ Success! Found the folder.\")\n",
    "    print(f\"Total files found in the folder: {len(all_npy_files)}\")\n",
    "\n",
    "    if len(all_npy_files) > 0:\n",
    "        print(\"\\nHere are the first 10 filenames found:\")\n",
    "        # Print the first 10 filenames for us to inspect\n",
    "        for filename in sorted(all_npy_files)[:10]:\n",
    "            print(filename)\n",
    "    else:\n",
    "        print(\"\\nWARNING: The folder is empty!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n❌ ERROR: The folder '{SPECTrogram_PATH}' does not exist.\")\n",
    "    print(\"Please double-check that you uploaded the folder and that the name is spelled exactly correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Advanced Training Regimen\n",
    "\n",
    "This script implements our most powerful training strategy. We introduce three key upgrades over the previous generalist model:\n",
    "\n",
    "1.  **Balanced Dataset:** We now load our data from pre-defined, balanced file lists. This ensures the model sees a proportional number of examples for each emotion, preventing bias and improving performance on under-represented classes.\n",
    "2.  **SpecAugment:** We apply a powerful data augmentation technique that randomly masks sections of time and frequency in our spectrograms. This forces the model to learn more robust and resilient features.\n",
    "3.  **Advanced Scheduler:** We use a `CosineAnnealingLR` scheduler, which intelligently adjusts the learning rate during training to help the model converge to a better final solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2120088,
     "status": "ok",
     "timestamp": 1756965017980,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "oHPsoMr0A22H",
    "outputId": "e017bf5c-9245-4bd3-b221-d0a556691063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading pre-defined and balanced data splits...\n",
      "Verifying that all spectrogram files exist...\n",
      "Train set: 11385 files found, 0 skipped.\n",
      "Validation set: 1266 files found, 0 skipped.\n",
      "Test set: 2233 files found, 0 skipped.\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 68.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting advanced training with SpecAugment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 [Train]: 100%|██████████| 178/178 [04:47<00:00,  1.61s/it]\n",
      "Epoch 1/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 | Train Loss: 1.0303 | Val Loss: 1.4354 | Val Acc: 57.35%\n",
      "🎉 New best validation accuracy: 57.35%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.27it/s]\n",
      "Epoch 2/40 [Val]: 100%|██████████| 20/20 [00:05<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40 | Train Loss: 0.6802 | Val Loss: 0.8935 | Val Acc: 67.77%\n",
      "🎉 New best validation accuracy: 67.77%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 [Train]: 100%|██████████| 178/178 [00:42<00:00,  4.15it/s]\n",
      "Epoch 3/40 [Val]: 100%|██████████| 20/20 [00:03<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 | Train Loss: 0.5857 | Val Loss: 1.3444 | Val Acc: 60.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.32it/s]\n",
      "Epoch 4/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40 | Train Loss: 0.4942 | Val Loss: 1.5582 | Val Acc: 55.13%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.32it/s]\n",
      "Epoch 5/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40 | Train Loss: 0.4041 | Val Loss: 0.6286 | Val Acc: 77.80%\n",
      "🎉 New best validation accuracy: 77.80%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 [Train]: 100%|██████████| 178/178 [00:43<00:00,  4.11it/s]\n",
      "Epoch 6/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40 | Train Loss: 0.3423 | Val Loss: 0.7170 | Val Acc: 74.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.24it/s]\n",
      "Epoch 7/40 [Val]: 100%|██████████| 20/20 [00:05<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40 | Train Loss: 0.2975 | Val Loss: 1.5647 | Val Acc: 63.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.25it/s]\n",
      "Epoch 8/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40 | Train Loss: 0.2288 | Val Loss: 0.9403 | Val Acc: 74.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.26it/s]\n",
      "Epoch 9/40 [Val]: 100%|██████████| 20/20 [00:03<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40 | Train Loss: 0.1965 | Val Loss: 0.8618 | Val Acc: 72.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.28it/s]\n",
      "Epoch 10/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40 | Train Loss: 0.1335 | Val Loss: 0.9757 | Val Acc: 75.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.34it/s]\n",
      "Epoch 11/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40 | Train Loss: 0.1175 | Val Loss: 1.0396 | Val Acc: 76.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.31it/s]\n",
      "Epoch 12/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40 | Train Loss: 0.1045 | Val Loss: 0.7833 | Val Acc: 80.09%\n",
      "🎉 New best validation accuracy: 80.09%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 [Train]: 100%|██████████| 178/178 [00:44<00:00,  4.04it/s]\n",
      "Epoch 13/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40 | Train Loss: 0.0845 | Val Loss: 0.7901 | Val Acc: 80.96%\n",
      "🎉 New best validation accuracy: 80.96%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 [Train]: 100%|██████████| 178/178 [00:43<00:00,  4.09it/s]\n",
      "Epoch 14/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40 | Train Loss: 0.0625 | Val Loss: 0.9422 | Val Acc: 79.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 [Train]: 100%|██████████| 178/178 [00:42<00:00,  4.23it/s]\n",
      "Epoch 15/40 [Val]: 100%|██████████| 20/20 [00:05<00:00,  3.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40 | Train Loss: 0.0521 | Val Loss: 0.9721 | Val Acc: 80.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.28it/s]\n",
      "Epoch 16/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40 | Train Loss: 0.0356 | Val Loss: 0.8286 | Val Acc: 81.28%\n",
      "🎉 New best validation accuracy: 81.28%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 [Train]: 100%|██████████| 178/178 [00:43<00:00,  4.12it/s]\n",
      "Epoch 17/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/40 | Train Loss: 0.0295 | Val Loss: 1.1224 | Val Acc: 79.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 [Train]: 100%|██████████| 178/178 [00:42<00:00,  4.21it/s]\n",
      "Epoch 18/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40 | Train Loss: 0.0427 | Val Loss: 0.8477 | Val Acc: 81.67%\n",
      "🎉 New best validation accuracy: 81.67%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 [Train]: 100%|██████████| 178/178 [00:43<00:00,  4.08it/s]\n",
      "Epoch 19/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40 | Train Loss: 0.0351 | Val Loss: 0.9897 | Val Acc: 80.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.24it/s]\n",
      "Epoch 20/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40 | Train Loss: 0.0147 | Val Loss: 0.8868 | Val Acc: 82.07%\n",
      "🎉 New best validation accuracy: 82.07%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 [Train]: 100%|██████████| 178/178 [00:42<00:00,  4.14it/s]\n",
      "Epoch 21/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40 | Train Loss: 0.0123 | Val Loss: 0.9049 | Val Acc: 82.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.33it/s]\n",
      "Epoch 22/40 [Val]: 100%|██████████| 20/20 [00:05<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40 | Train Loss: 0.0182 | Val Loss: 0.9695 | Val Acc: 81.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.30it/s]\n",
      "Epoch 23/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 | Train Loss: 0.0138 | Val Loss: 1.0451 | Val Acc: 80.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.28it/s]\n",
      "Epoch 24/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40 | Train Loss: 0.0116 | Val Loss: 0.9176 | Val Acc: 81.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 [Train]: 100%|██████████| 178/178 [00:40<00:00,  4.34it/s]\n",
      "Epoch 25/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/40 | Train Loss: 0.0137 | Val Loss: 0.9317 | Val Acc: 82.31%\n",
      "🎉 New best validation accuracy: 82.31%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 [Train]: 100%|██████████| 178/178 [00:42<00:00,  4.21it/s]\n",
      "Epoch 26/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40 | Train Loss: 0.0106 | Val Loss: 0.9337 | Val Acc: 81.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.33it/s]\n",
      "Epoch 27/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40 | Train Loss: 0.0095 | Val Loss: 0.9309 | Val Acc: 81.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.30it/s]\n",
      "Epoch 28/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/40 | Train Loss: 0.0062 | Val Loss: 1.0236 | Val Acc: 81.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 [Train]: 100%|██████████| 178/178 [00:40<00:00,  4.35it/s]\n",
      "Epoch 29/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40 | Train Loss: 0.0048 | Val Loss: 0.9519 | Val Acc: 82.78%\n",
      "🎉 New best validation accuracy: 82.78%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 [Train]: 100%|██████████| 178/178 [00:43<00:00,  4.13it/s]\n",
      "Epoch 30/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/40 | Train Loss: 0.0044 | Val Loss: 0.9349 | Val Acc: 83.02%\n",
      "🎉 New best validation accuracy: 83.02%. Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 [Train]: 100%|██████████| 178/178 [00:43<00:00,  4.14it/s]\n",
      "Epoch 31/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40 | Train Loss: 0.0035 | Val Loss: 0.9416 | Val Acc: 82.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 [Train]: 100%|██████████| 178/178 [00:42<00:00,  4.20it/s]\n",
      "Epoch 32/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/40 | Train Loss: 0.0040 | Val Loss: 0.9845 | Val Acc: 81.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.27it/s]\n",
      "Epoch 33/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40 | Train Loss: 0.0026 | Val Loss: 0.9538 | Val Acc: 82.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.27it/s]\n",
      "Epoch 34/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40 | Train Loss: 0.0020 | Val Loss: 0.9455 | Val Acc: 82.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.31it/s]\n",
      "Epoch 35/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40 | Train Loss: 0.0020 | Val Loss: 0.9849 | Val Acc: 82.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.33it/s]\n",
      "Epoch 36/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40 | Train Loss: 0.0028 | Val Loss: 0.9630 | Val Acc: 83.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.28it/s]\n",
      "Epoch 37/40 [Val]: 100%|██████████| 20/20 [00:03<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/40 | Train Loss: 0.0018 | Val Loss: 0.9677 | Val Acc: 82.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.32it/s]\n",
      "Epoch 38/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40 | Train Loss: 0.0012 | Val Loss: 0.9515 | Val Acc: 82.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 [Train]: 100%|██████████| 178/178 [00:40<00:00,  4.35it/s]\n",
      "Epoch 39/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/40 | Train Loss: 0.0035 | Val Loss: 0.9822 | Val Acc: 82.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 [Train]: 100%|██████████| 178/178 [00:41<00:00,  4.33it/s]\n",
      "Epoch 40/40 [Val]: 100%|██████████| 20/20 [00:04<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40 | Train Loss: 0.0015 | Val Loss: 0.9657 | Val Acc: 81.91%\n",
      "\n",
      "--- FINAL EVALUATION OF ADVANCED GENERALIST MODEL ---\n",
      "Loading best model (from epoch with 83.02% validation accuracy) for final testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Evaluation: 100%|██████████| 35/35 [00:07<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Advanced Generalist Model Accuracy on the Test Set: 81.01%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.79      0.82      0.81       266\n",
      "       happy       0.83      0.81      0.82       374\n",
      "         sad       0.76      0.83      0.80       393\n",
      "       angry       0.84      0.86      0.85       383\n",
      "     fearful       0.83      0.74      0.78       390\n",
      "     disgust       0.82      0.80      0.81       427\n",
      "\n",
      "    accuracy                           0.81      2233\n",
      "   macro avg       0.81      0.81      0.81      2233\n",
      "weighted avg       0.81      0.81      0.81      2233\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ULTIMATE COLAB SCRIPT v8: The Advanced Generalist Trainer (FINAL PATH FIX 3)\n",
    "# ===================================================================\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, numpy as np, pickle\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "# --- Configuration ---\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms_final/\"\n",
    "FILE_LIST_PATH = \"/content/drive/MyDrive/ser_project/\"\n",
    "LEARNING_RATE = 0.001; BATCH_SIZE = 64; EPOCHS = 40\n",
    "CHECKPOINT_BEST_PATH = \"/content/drive/MyDrive/ser_project/resnet_advanced_best.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Mappings ---\n",
    "unified_emotion_map = { \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3, \"fearful\": 4, \"disgust\": 5 }\n",
    "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
    "\n",
    "# --- SpecAugment Transformation Pipeline ---\n",
    "spec_augment_transform = transforms.Compose([\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.05), ratio=(0.2, 5.0), value=0),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.08), ratio=(0.01, 0.2), value=0),\n",
    "])\n",
    "\n",
    "# --- Helper function to get filename robustly ---\n",
    "def get_basename(path):\n",
    "    return path.replace('\\\\', '/').split('/')[-1]\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300):\n",
    "        self.file_paths, self.labels, self.target_width = file_paths, labels, target_width\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        # Use our new robust function to get the base filename\n",
    "        filename = get_basename(self.file_paths[idx]).replace('.wav', '.npy')\n",
    "        file_path = os.path.join(SPECTROGRAM_PATH, filename)\n",
    "        label = self.labels[idx]\n",
    "        spectrogram = np.load(file_path)\n",
    "        current_width = spectrogram.shape[1]\n",
    "        if current_width < self.target_width: spectrogram = np.pad(spectrogram, ((0, 0), (0, self.target_width - current_width)), mode='constant')\n",
    "        elif current_width > self.target_width: spectrogram = spectrogram[:, :self.target_width]\n",
    "        spec_min, spec_max = spectrogram.min(), spectrogram.max()\n",
    "        if spec_max > spec_min: spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram_3ch = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "        return torch.tensor(spectrogram_3ch, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Prepare Data ---\n",
    "print(\"Loading pre-defined and balanced data splits...\")\n",
    "with open(os.path.join(FILE_LIST_PATH, 'train_files.pkl'), 'rb') as f: train_files_raw = pickle.load(f)\n",
    "with open(os.path.join(FILE_LIST_PATH, 'val_files.pkl'), 'rb') as f: val_files_raw = pickle.load(f)\n",
    "with open(os.path.join(FILE_LIST_PATH, 'test_files.pkl'), 'rb') as f: test_files_raw = pickle.load(f)\n",
    "\n",
    "print(\"Verifying that all spectrogram files exist...\")\n",
    "def verify_and_filter_files(file_list_raw):\n",
    "    verified_files = []\n",
    "    for f_path in file_list_raw:\n",
    "        # Use our new robust function here as well\n",
    "        npy_filename = get_basename(f_path).replace('.wav', '.npy')\n",
    "        full_npy_path = os.path.join(SPECTROGRAM_PATH, npy_filename)\n",
    "        if os.path.exists(full_npy_path):\n",
    "            # We keep the original path in the list for the label getter\n",
    "            verified_files.append(f_path)\n",
    "    skipped_count = len(file_list_raw) - len(verified_files)\n",
    "    return verified_files, skipped_count\n",
    "\n",
    "train_files, train_skipped = verify_and_filter_files(train_files_raw)\n",
    "val_files, val_skipped = verify_and_filter_files(val_files_raw)\n",
    "test_files, test_skipped = verify_and_filter_files(test_files_raw)\n",
    "\n",
    "print(f\"Train set: {len(train_files)} files found, {train_skipped} skipped.\")\n",
    "print(f\"Validation set: {len(val_files)} files found, {val_skipped} skipped.\")\n",
    "print(f\"Test set: {len(test_files)} files found, {test_skipped} skipped.\")\n",
    "\n",
    "# Create label lists\n",
    "ravdess_map = { \"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\" }\n",
    "crema_d_map = { \"NEU\": \"neutral\", \"HAP\": \"happy\", \"SAD\": \"sad\", \"ANG\": \"angry\", \"FEA\": \"fearful\", \"DIS\": \"disgust\" }\n",
    "def get_label_from_path(filepath):\n",
    "    filename = get_basename(filepath)\n",
    "    try:\n",
    "        if '03-01' in filename: return unified_emotion_map[ravdess_map[filename.split(\"-\")[2]]]\n",
    "        else: return unified_emotion_map[crema_d_map[filename.split(\"_\")[2]]]\n",
    "    except (IndexError, KeyError): return None\n",
    "\n",
    "train_labels = [get_label_from_path(f) for f in train_files]; val_labels = [get_label_from_path(f) for f in val_files]; test_labels = [get_label_from_path(f) for f in test_files]\n",
    "\n",
    "train_dataset = SpectrogramDataset(train_files, train_labels); val_dataset = SpectrogramDataset(val_files, val_labels); test_dataset = SpectrogramDataset(test_files, test_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2); val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2); test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# --- Train the Model ---\n",
    "model = models.resnet18(weights='IMAGENET1K_V1'); model.fc = nn.Linear(model.fc.in_features, len(unified_emotion_labels)); model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE); criterion = nn.CrossEntropyLoss()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "best_val_acc = 0.0\n",
    "print(\"Starting advanced training with SpecAugment...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(); running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = spec_augment_transform(inputs)\n",
    "        optimizer.zero_grad(); outputs = model(inputs); loss = criterion(outputs, labels)\n",
    "        loss.backward(); optimizer.step(); running_loss += loss.item() * inputs.size(0)\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    model.eval(); val_loss = 0.0; correct = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Val]\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs); loss = criterion(outputs, labels); val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1); total += labels.size(0); correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = 100 * correct / total; val_loss /= len(val_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "    if val_accuracy > best_val_acc:\n",
    "        best_val_acc = val_accuracy\n",
    "        print(f\"🎉 New best validation accuracy: {best_val_acc:.2f}%. Saving model...\")\n",
    "        torch.save({'model_state_dict': model.state_dict()}, CHECKPOINT_BEST_PATH)\n",
    "    scheduler.step()\n",
    "\n",
    "# --- Final Evaluation ---\n",
    "print(\"\\n--- FINAL EVALUATION OF ADVANCED GENERALIST MODEL ---\")\n",
    "print(f\"Loading best model (from epoch with {best_val_acc:.2f}% validation accuracy) for final testing...\")\n",
    "best_checkpoint = torch.load(CHECKPOINT_BEST_PATH); model.load_state_dict(best_checkpoint['model_state_dict']); model.eval()\n",
    "all_preds, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Final Evaluation\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs); _, preds = torch.max(outputs, 1); all_preds.extend(preds.cpu().numpy()); all_true.extend(labels.cpu().numpy())\n",
    "accuracy = accuracy_score(all_true, all_preds)\n",
    "print(f\"\\nFinal Advanced Generalist Model Accuracy on the Test Set: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\"); print(classification_report(all_true, all_preds, target_names=unified_emotion_labels, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Final Verdict\n",
    "\n",
    "This is the definitive evaluation of our champion CNN-based model. After training with our advanced regimen, we load the best-performing checkpoint and evaluate it separately on the RAVDESS and CREMA-D test sets. This provides a clear measure of its mastery of the clean domain and its generalization capability on the challenging domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9986,
     "status": "ok",
     "timestamp": 1756965396913,
     "user": {
      "displayName": "Monsur",
      "userId": "05915807458129171381"
     },
     "user_tz": -360
    },
    "id": "L7It-IhySoC5",
    "outputId": "3b1f6cbc-3c81-427e-987c-47dc419bcb70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading the best 'Ultimate Generalist' model...\n",
      "Loading the test set data split...\n",
      "File paths normalized for Linux environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on RAVDESS Test Set: 100%|██████████| 18/18 [00:04<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Accuracy on RAVDESS Test Set: 99.73%\n",
      "Classification Report for RAVDESS Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.97      1.00      0.98        97\n",
      "       happy       1.00      0.99      0.99       202\n",
      "         sad       1.00      1.00      1.00       198\n",
      "       angry       1.00      1.00      1.00       188\n",
      "     fearful       1.00      1.00      1.00       203\n",
      "     disgust       1.00      1.00      1.00       216\n",
      "\n",
      "    accuracy                           1.00      1104\n",
      "   macro avg       0.99      1.00      1.00      1104\n",
      "weighted avg       1.00      1.00      1.00      1104\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating on CREMA-D Test Set: 100%|██████████| 18/18 [00:04<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Accuracy on CREMA-D Test Set: 62.71%\n",
      "Classification Report for CREMA-D Test Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral       0.69      0.71      0.70       169\n",
      "       happy       0.62      0.61      0.62       172\n",
      "         sad       0.56      0.67      0.61       195\n",
      "       angry       0.69      0.72      0.70       195\n",
      "     fearful       0.59      0.47      0.52       187\n",
      "     disgust       0.62      0.59      0.61       211\n",
      "\n",
      "    accuracy                           0.63      1129\n",
      "   macro avg       0.63      0.63      0.63      1129\n",
      "weighted avg       0.63      0.63      0.62      1129\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# FINAL SCRIPT v2: Evaluating the Generalist Model (with Path Fix)\n",
    "# ===================================================================\n",
    "import torch, torch.nn as nn, os, numpy as np, pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "\n",
    "# --- Configuration ---\n",
    "SPECTROGRAM_PATH = \"/content/drive/MyDrive/ser_project/processed_spectrograms_final/\"\n",
    "FILE_LIST_PATH = \"/content/drive/MyDrive/ser_project/\"\n",
    "CHECKPOINT_BEST_PATH = \"/content/drive/MyDrive/ser_project/resnet_advanced_best.pth\"\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"); print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Mappings and Dataset Class ---\n",
    "unified_emotion_map = { \"neutral\": 0, \"happy\": 1, \"sad\": 2, \"angry\": 3, \"fearful\": 4, \"disgust\": 5 }\n",
    "unified_emotion_labels = [\"neutral\", \"happy\", \"sad\", \"angry\", \"fearful\", \"disgust\"]\n",
    "\n",
    "def get_basename(path): # Robust way to get filename\n",
    "    return path.replace('\\\\', '/').split('/')[-1]\n",
    "\n",
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, target_width=300):\n",
    "        self.file_paths, self.labels, self.target_width = file_paths, labels, target_width\n",
    "    def __len__(self): return len(self.file_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        filename = get_basename(self.file_paths[idx]).replace('.wav', '.npy')\n",
    "        file_path = os.path.join(SPECTROGRAM_PATH, filename)\n",
    "        label = self.labels[idx]\n",
    "        spectrogram = np.load(file_path)\n",
    "        current_width = spectrogram.shape[1]\n",
    "        if current_width < self.target_width: spectrogram = np.pad(spectrogram, ((0, 0), (0, self.target_width - current_width)), mode='constant')\n",
    "        elif current_width > self.target_width: spectrogram = spectrogram[:, :self.target_width]\n",
    "        spec_min, spec_max = spectrogram.min(), spectrogram.max()\n",
    "        if spec_max > spec_min: spectrogram = (spectrogram - spec_min) / (spec_max - spec_min)\n",
    "        spectrogram_3ch = np.stack([spectrogram, spectrogram, spectrogram], axis=0)\n",
    "        return torch.tensor(spectrogram_3ch, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Load the Best Trained Model ---\n",
    "print(\"Loading the best 'Ultimate Generalist' model...\")\n",
    "model = models.resnet18(); model.fc = nn.Linear(model.fc.in_features, len(unified_emotion_labels));\n",
    "best_checkpoint = torch.load(CHECKPOINT_BEST_PATH); model.load_state_dict(best_checkpoint['model_state_dict']);\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- Load the test set file list ---\n",
    "print(\"Loading the test set data split...\")\n",
    "with open(os.path.join(FILE_LIST_PATH, 'test_files.pkl'), 'rb') as f: test_files_raw = pickle.load(f)\n",
    "\n",
    "# --- THIS IS THE CRUCIAL FIX ---\n",
    "# Normalize Windows paths ('\\') to Linux paths ('/')\n",
    "test_files = [p.replace('\\\\', '/') for p in test_files_raw]\n",
    "print(\"File paths normalized for Linux environment.\")\n",
    "\n",
    "# --- Create the label list for the test set ---\n",
    "ravdess_map = { \"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\", \"06\": \"fearful\", \"07\": \"disgust\" }\n",
    "crema_d_map = { \"NEU\": \"neutral\", \"HAP\": \"happy\", \"SAD\": \"sad\", \"ANG\": \"angry\", \"FEA\": \"fearful\", \"DIS\": \"disgust\" }\n",
    "def get_label(filepath):\n",
    "    filename = get_basename(filepath)\n",
    "    try:\n",
    "        if '03-01' in filename: return unified_emotion_map[ravdess_map[filename.split(\"-\")[2]]]\n",
    "        else: return unified_emotion_map[crema_d_map[filename.split(\"_\")[2]]]\n",
    "    except (IndexError, KeyError): return None\n",
    "test_labels = [get_label(f) for f in test_files]\n",
    "\n",
    "# Filter out any files that might have failed label parsing\n",
    "valid_indices = [i for i, lbl in enumerate(test_labels) if lbl is not None]\n",
    "test_files = [test_files[i] for i in valid_indices]\n",
    "test_labels = [test_labels[i] for i in valid_indices]\n",
    "\n",
    "# --- Filter the test set for each dataset ---\n",
    "ravdess_test_files = [f for f in test_files if 'ravdess_data' in f.lower()]\n",
    "ravdess_test_labels = [l for i, l in enumerate(test_labels) if 'ravdess_data' in test_files[i].lower()]\n",
    "\n",
    "crema_d_test_files = [f for f in test_files if 'crema_d_data' in f.lower()]\n",
    "crema_d_test_labels = [l for i, l in enumerate(test_labels) if 'crema_d_data' in test_files[i].lower()]\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate(files, labels, name):\n",
    "    dataset = SpectrogramDataset(files, labels)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    all_preds, all_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labs in tqdm(loader, desc=f\"Evaluating on {name}\"):\n",
    "            inputs, labs = inputs.to(device), labs.to(device)\n",
    "            outputs = model(inputs); _, preds = torch.max(outputs, 1); all_preds.extend(preds.cpu().numpy()); all_true.extend(labs.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_true, all_preds)\n",
    "    print(f\"\\n>>> Accuracy on {name}: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Classification Report for {name}:\"); print(classification_report(all_true, all_preds, target_names=unified_emotion_labels, zero_division=0))\n",
    "\n",
    "# --- Run the Final Evaluations ---\n",
    "if ravdess_test_files:\n",
    "    evaluate(ravdess_test_files, ravdess_test_labels, \"RAVDESS Test Set\")\n",
    "if crema_d_test_files:\n",
    "    evaluate(crema_d_test_files, crema_d_test_labels, \"CREMA-D Test Set\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNPamPOL5uOuzNlT41cTLk8",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
